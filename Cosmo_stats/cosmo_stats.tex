\documentclass[onecolumn,           % Format : preprint, twocolumn
%\documentclass[preprint,           % Format : preprint, twocolumn
               showpacs,            % Pacs : showpacs, noshowpacs
               preprintnumbers,     % Preprint: preprintnumbers,
               			    %           nopreprintnumbers
               aps,                 % Society: ...
               prl,          	    % Journal Style : pra, prb, prc, prd, pre,
               			    %                 prl, prstab, rmp
               letterpaper,             % Size : a4paper, ...
               superscriptaddress,      % Affiliation (Title) : groupedaddress,
                                    %                       superscriptaddress,
                                    %                       unsortedaddress
               nofootinbib,         % Footnote: footinbib, nofootinbib
               tightenlines,        % Remove additional spaces in a line
               floats,floatfix      % Floating pictures and tables
               ,usenatbib,
               ]{revtex4-1}
\usepackage{graphicx}  % needed for figures
\usepackage{dcolumn}   % needed for some tables}
%\usepackage[style=authoryear,backend=biber]{biblatex}
\usepackage{bm}        % for math
\usepackage{amsmath,amssymb}
\usepackage{hyperref}
\usepackage{color}
\definecolor{purple}{rgb}{0.58,0.0,0.83}
\usepackage{caption}
\usepackage[toc,page]{appendix}

\begin{document}

\title{An introduction on bayesian parameter inference and its applications in cosmology}
\author{Luis Padilla-Albores}  
\email{epadilla@fis.cinvestav.mx}
\affiliation{Departamento de F\'isica, Centro de Investigaci\'on y de Estudios Avanzados del IPN, A.P. 14-740, 07000 M\'exico D.F.,
  M\'exico.}
   \author{Alberto Vazquez-Gonzalez}  
\email{vetovazquez@hotmail.com}
\affiliation{Departamento de F\'isica, Centro de Investigaci\'on y de Estudios Avanzados del IPN, A.P. 14-740, 07000 M\'exico D.F.,
  M\'exico.}
  \author{Luis O. Tellez}  
\email{ltellez@fis.cinvestav.mx}
\affiliation{Departamento de F\'isica, Centro de Investigaci\'on y de Estudios Avanzados del IPN, A.P. 14-740, 07000 M\'exico D.F.,
  M\'exico.}
\date{\today}

\begin{abstract}

In this paper we review the basic concepts on Bayesian statistics for parameter inference and how we can use it in cosmology. This work is organized in such a way that if the reader is only interested in the parameter inference procedure for a model, but not in the cosmological part, he/she can make use of it ignoring only the last part of the paper. 

First, we start by giving the basic differences between Bayesian and Frequentist statistics. Then we continue with a not so formal introduction to the basic mathematical concepts necessary for a Bayesian parameter inference procedure. Later, we review the most common computational tools at our disposal that can help us to simplify this task. Finally we show how this new concepts can be used in cosmology.     
\end{abstract}

%\pacs{????}
\maketitle

\section{Introduction}

The beginning of the standard cosmology as it is known today emerged after 1920 when the Shaphey-Curtis debate was carried out \cite{debate}. This debate was held between the astronomers Harlow Shapley and Heber Curtis, resulting in a revolution for astronomy at that time: ``The universe had a larger scale than the Milky Way galaxy". Several observations at that epoch established the size and the dynamics of the cosmos that could only be explained by Einstein's General Theory of Relativity. In its childhood, Cosmology was a speculative science that was based only on a few data sets and characterized by a dispute between two cosmological models: the steady state model and the Big Bang (BB) theory. It was not until 1990 when the amount of data was enough to eliminate competing theories, being awarded the BB model as the most accepted cosmological theory. In that same decade David Schramm heralded the ``Golden Age of Cosmology" at a National Academy of Sciences colloquium.    

Once the new age of cosmological experiments arrived with a very large variety of cosmological data, it was necessary to confront all the cosmological models with said data. The way that it is usually done is using a statistical confrontation. First of all we need to notice that, since we only have one Universe, we can not consider a frequentist interpretation of statistics (we can not create multiple Universes to make a frequentist inference of our models). An alternative interpretation that can help us is the Bayesian statistics. In Bayesian statistics probability is interpreted as a ``degree of belief'' and it can be useful when no repetitive processes need to be considered.  

The main objective of this work is to give the reader an introduction on bayesian parameter inference and how we can use it in cosmology. We assume that the reader is familiarized with the basic concepts of statistics, but not necessarily with Bayesian statistics. Then, we give a not so formal general view of it, enough to understand the basic concepts of our main objective. This general view is written in a generic way so that if the reader is not interested in the cosmological section but in the parameter inference one he/she can draw on the bayesian part.  

This paper is organized as follows. We start in the section [Bayesian vs Frequentist statistic] mentioning the most important differences between the Bayesian statistics and the Frequentist one. Then, in section [A first look on the Bayesian statistics] we present the basic mathematical concepts in Bayesian statistics that are going to be necessary at the moment that we want to estimate a model parameter. Once we have the mathematical concepts, we continue in section [Numerical tools] with the numerical tools at our disposal that can help us simplify our homework. Such numerical tools are very important given the fact that, in general, it is not possible to apply the analytical ones when several parameters of our models need to be confronted with data. In section [Bayesian statistics and cosmology] we show how these tools can be used in cosmology, mentioning the different numerical codes free to download on web that are programmed to do this homework and applying them to specific examples. Finally, in section [Conclusions] we conclude this paper.

\section{Bayesian vs Frequentist statistics}

Fundamentally, the main difference between Bayesian and Frequentist statistics is their definition of probability. In a frequentist point of view probability has meaning in a limiting case of repeated measurements
\begin{equation}
P=\frac{n}{N}
\end{equation}
where $n$ denotes the number of successes and $N$ the total number of attempts. Frequentist statistics define probability as the limit for the number of independent trials going to infinity \textcolor{red}{(cambiar definicion, suena rara)}. Then, \textbf{for frequentist statistics, probabilities are fundamentally related to frequencies of events}. On the other hand, in bayesian statistics the concept of probability is extended to cover degrees of certainty about a statement. \textbf{For Bayesian statistics, probabilities are fundamentally related to our knowledge about an event}.

On this part we introduce the basic concepts necessary to understand the basic consequences that this discrepancy entails. For an extended review see \cite{bayeslecture}, \cite{AlanH}, \cite{RobT}, \cite{LiV} or/and \cite{RobTr}. 

If we consider that $x$ is a random variable related to a particular event and $P(x)$ its corresponding probability distribution, for both cases the same rules of probabilities apply\footnote{This rules are defined for a continuous variable; however, the corresponding discrete definition can be given immediately by replacing $\int \rightarrow \sum$.}:
\begin{subequations}\label{rules}
\begin{equation}\label{rule1}
P(x)\geq 0
\end{equation}
\begin{equation}\label{rule2}
\int_{-\infty}^\infty dxP(x)=1\end{equation}
For mutually exclusive events \begin{equation}\label{rule3}
P(x_1\cup x_2)=P(x_1)+P(x_2)
\end{equation}
Generally
\begin{equation}\label{rule4}
P(x_1,x_2)=P(x_1)P(x_1|x_2)\end{equation}
\end{subequations}
The last rule can be read as: the probability of $x_1$ and $x_2$ to happen is equal to the probability of $x_1$ times the probability of $x_2$ given that $x_1$ has already happened. \\
%But what does the last rules means? The first condition is necessary because we want that the probability of having an event is always positive. The second rule is a normalized relation which tell us that whatever event we have, it will always be a corresponding probability in the model that we are working with. Now, in the third point we have a relation about two mutually exclusive events which can be reed as follows: if $x_1$ and $x_2$ are two measurements one independent of the other, then 


These rules for probability distributions must be fulfilled by both frequentist and bayesian statistics. But what are the consequences derived by the fact that these two scenarios have a different definition of probability? In the next section we will try to answer this question.

\subsection{Frequentist statistics}

Any frequentist inferential procedure relies on three basic ingredients: the data, a model and an estimation procedure. The main assumption in Frequentist statistics is that the data has a definite, albeit unknown, underlying distribution to which all inference pertains.

The \textit{data} is a measurement or observation, denoted by $X$, that can take any value from a corresponding \textit{sample space}. A \textit{sample space} of an observation $X$ can be defined as a measurable space $(x,\hat B)$ that contains all values that $X$ can take upon measurement.

In Frequentist statistics it is considered that there is a probability function $P_0:\hat B\rightarrow [0,1]$ in the sample space $(x,\hat B)$ representing the ``true distribution of the data"
\[X\sim P_0\]

Now we have the model. For Frequentist statistics a model $Q$ is a collection of probability measurements $P:\hat B\rightarrow[0,1]$ in the sample space $(x,\hat B)$. The distributions $P_\theta$ are called model distributions. In this approach $\theta$ is unchanged. 

A model $Q$ is said to be well-specified if it contains the true distribution of the data $P_0$, i.e.
\[P_0\in Q\]

Finally, we need a point-estimator(or estimator) for $P_0$. An estimator for $P_0$ is a map $\hat P:x\rightarrow Q$, representing our ``best guess" $\hat P\in Q$ for $P_0$ based on the data $X$.

Hence, the Frequentist statistics is based on trying to answer the following questions: ``what does the data clarify about $P_0$?" or ``from the data, what can we say about the mean of $P_0$?".

\subsection{Bayesian statistics}

As it is explained in \cite{bayeslecture}, in bayesian statistics, data and model form two elements of the same space, i.e. no formal distinction is made between measured quantities $X$ and parameters $\theta$. One may envisage the process of generating a measurement's outcome $Y=y$ as two draws, one draw for $\Theta$ (or $Q$) to select a value of $\theta$ (or distribution $P_\theta$) and a subsequent draw for $P_\theta$ to arrive at $X=x$ \textcolor{red}{(que es draw en este contexto?)} \textcolor{blue}{Ser\'ia algo similar a trazo}. This perspective may seem rather absurd in view of the definitions for a frequentist way of thinking, but in a bayesian one, where probabilities are related to our own knowledge, it results natural to associate probability distributions to our parameters. In this way an element $P_\theta$ of the model is interpreted simply as the distribution of $X$ given the parameter value $\theta$, i.e. as the conditional distribution $X|\theta$.
\subsection{A bit clearer}

It is very important to understand the differences between both approximations. For it, let us review in this subsection the mandatory example that can help us to easily understand these differences.
\begin{table}[h!]
\centering
\begin{tabular}{||l|l||} 
 \hline
 \textbf{Frequentist} & \textbf{Bayesian} \\ [0.5ex] 
 \hline\hline
 Data are a repeatable random  & Data are observed from the   \\ 
 sample. There is a frequency & realized sample \\
 \hline 
 Underlying parameters remain & Parameters are unknown and \\
 constant during this repeatable & described probabilistically \\
 process &  \\
\hline
Parameters are fixed & Data are fixed\\ [1ex] 
 \hline
\end{tabular}
\caption{\footnotesize{Main differences between the bayesian and Frequentist interpretations.}}
\label{table:1}
\end{table}

In table \ref{table:1} we have a little review of the most important differences about the two approximations. In the next example we present an experiment seen from both points of view. Since we are interested in knowing both descriptions, we show only the basic results of the procedures and we analyze them in the point of view of both the Frequentist and Bayesian statistics. 

\textit{Example.-} In this experiment we have a coin that has a probability $p$ to land as heads and a probability $1-p$ to land tails. Trying to estimate $p$ (that must be $p=0.5$ since we have two possible states) we flip the coin 14 times, obtaining heads in 10 of them. Now we are interested in the next two possible events. To be precise: ``What is the probability that in the next two tosses we will get two heads in a row?"
\begin{itemize}
\item \textit{Frequentist approximation}. As mentioned previously, in Frequentist statistics probability is related to the frequency of events, then our best estimate for $p$ is $P(head)=p=$No.heads/No.total$=10/14$. So, the probability of having 2 heads in a row is $P(2heads)=P(head)P(head)\simeq 0.51$.  
\item \textit{Bayesian approximation}. In the bayesian approach $p$ is not a value, it is a random variable with its own distribution. This distribution must be defined by the existing evidence. In this example a good distribution for $p$ is a binomial distribution. Then, by considering a uniform distribution as our prior information (we do not know anything about $p$) we have that the probability of having two heads is
\[
P(2heads|D)=\frac{B(13,5)}{B(11,5)}=0.485
\]
where $B(x,y)$ is the beta function. This distribution appears given the fact that $B(1,1)$ is the uniform distribution which we consider as our prior \textcolor{red}{(reescribir esta oracion, pero no se me ocurre como :,c)}.
\end{itemize}

We can see that both approximations arrive at different results. In one of them, since we consider probability as a frequency of events we have a bigger probability to have two heads in a row than when we consider probability as our knowledge about the experiment, however, in both cases, the probability differs from the real one ($P(2heads)=0.25$) because we don't have enough data for our estimations.

\textit{Note}: If you are not familiarized with Bayesian statistics, please don't be scared of the last example. In the next section we will introduce the basic concepts to understand the bayesian part and then we will go back to this example to solve it using the new tools acquired.   

\textcolor{red}{(Tratar de escribir esta parte un poco m\'as bonito)}
\\

\textcolor{blue}{De hecho creo que ser\'ia mejor si se quita esta parte}
%\subsection{Summary}

%In this section we reviewed the basic differences between the Bayesian and the Frequentist interpretation of statistics. Basically, what is important here is to notice that probability has a different meaning for a frequentist statistician than for a bayesian one. While for a Frequentist, probabilities are related to frequencies of events, for Bayesians, probabilities are related to our own knoledge about an event. Then, given this basic difference, there are several other ones that are a result of it and that are well summarized in table  \ref{table:1}. 

\section{A first look at Bayesian statistics}

Before we start with the applications of bayesian statistics in cosmology it's necessary to learn the most important tools for the bayesian analysis. In this section, we review them in an informal way, keeping in mind that the reader can look for the formal treatment in literature.   

\subsection{Bayes theorem, priors, posteriors and all that stuff}\label{BTPP}

When a statistician is interested in working with the Bayes framework, there are several concepts that are necessary to understand before he can interpret the results obtained. In this section we quickly review these concepts and then we take back the example about the coin toss given in the last section. 

\textit{The Bayes theorem.} The Bayes theorem is a direct consequence of the axioms of probability \eqref{rules}. We can see that, from \eqref{rule4}, without loss of generality, we can rewrite $P(x_2,x_1)=P(x_2)P(x_2|x_1)$. Considering that the relation $P(x_1,x_2)=P(x_2,x_1)$ has to be fulfilled, we arrive at the Bayes formula   
\begin{equation}
P(x_2|x_1)=\frac{P(x_2)P(x_1|x_2)}{P(x_1)}
\end{equation}
As it was mentioned, in the Bayes framework, data and model are part of the same space. In this manner, considering $x_1\rightarrow D$ as a set of data, and $x_2\rightarrow H$ as our "hypothesis", we can rewrite the above equation in Bayesian statistics as
\begin{equation}\label{BayesT}
P(\theta,H|D)=\frac{P(\theta,H)P(D|\theta,H)}{P(D)}
\end{equation}
Here we have added the extra term $\theta$ in order to specify that our hypothesis can depend on one or several parameters. 
The above equation is the so-called \textit{Bayes theorem} and it's the most important tool in a Bayesian inference procedure. In this result, $P(\theta,H|D)$ is called the \textit{posterior} probability for the model. $L(D;\theta)\equiv P(D|\theta,H)$ is called the \textit{Likelihood} and it will be our main focus in a future section. $P(D|\theta,H)$ is called the \textit{prior} and expresses what we know about our model before acquiring the data. This prior can be fixed depending on either previous experiment results or the theory that we are working with. $P(D)$ is the evidence of our model. We notice that this evidence acts only as a normalizing factor
\begin{equation}\label{PD}
P(D)=\int d\theta P(D|\theta,H)P(\theta,H).
\end{equation}
This quantity is usually ignored for practical reasons when testing the parameter space of a unique model. On the other side, if what we are interested in is a comparison between 2 or more models, the evidence plays an important role when we choose which model is more likely to be the ``real one'', whatever the parameters are. For the purpose of this work, we are not interested in this scenario and we will ignore it at all times.

The reader can notice that we have added here a new ingredient in our Bayes description: the hypothesis. An hypothesis is our best guess at which model best fits the data, $H=Q_{best}$.

We can see that Bayes theorem has an enormous implication with respect to an statistical inferential point of view. In a typical scenario we can collect some data and hope to interpret it with a given model, however, what we can usually do is the opposite, that is, first we have a set of data and then we can confront a model considering what is the probability that our model fits the data. As we can see from \eqref{BayesT}, Bayes theorem gives us a tool that allows us to relate both scenarios. Then, thanks to the Bayes theorem, in principle, we can know what is the ``real" model that best fits the data. 

\textit{Example}.- We consider again the example shown in the last section: the coin toss. What we are interested in is knowing what is the probability $P(2heads|D)$ ($D=$ the previous 14 coin tosses acting as data) to have 2 heads in a row given the data. For simplicity, we do not update $p$ between the two tosses and we assume that both tosses are independent from each other. To obtain the result that we expect we need to calculate
\begin{equation}\label{ex}
P(2heads|D)=\int^1_0 P(2heads|p)P(p|D)dp
\end{equation}
where $P(p|D)$ is the probability of the data given our model $p$ and $P(2heads|p)$ is the probability of our model $p$ given that we had 2 heads. Since we are considering that both tries are independent we have
\begin{equation}
P(2heads|p)=[P(head|p)]^2
\end{equation}
where $P(head|p)$ is the probability of our model $p$ given that we obtained heads once. A good model for our experiment, given the data, is a binomial distribution
\begin{equation}\label{ex2}
P(D|p)=\binom{14}{10}p^{10}(1-p)^4
\end{equation}
Then 
\begin{equation}\label{ex1}
P(head|p)=p\Rightarrow P(2head|p)=p^2
\end{equation}
Now we need to compute $P(p|D)$. Using the bayes theorem we have
\begin{equation}
P(p|D)=\frac{P(D|p)P(p)}{P(D)}
\end{equation}
A very convenient prior distribution for this scenario is the \textit{beta distribution} $Beta(p;a,b)$ defined as
\begin{equation}\label{ex3}
Beta(p;a,b)=\frac{\Gamma(a+b)}{\Gamma(a)\Gamma(b)}p^{a-1}(1-p)^{b-1}
\end{equation}
where $\Gamma$ is the gamma function. So
\begin{equation}\label{ex4}
P(p)=Beta(p;a,b)
\end{equation}
We are interested in the explicit form of $P(p|D)$ in such case we need to compute $P(D)$. Introducing \eqref{ex2} and \eqref{ex4} into \eqref{PD} we have
\begin{equation}
P(D)=B(10+a,4+b)\equiv \frac{\Gamma(10+a)\Gamma(4+b)}{\Gamma((10+a)+(4+b))}
\end{equation}
and then
\begin{equation}\label{ex5}
P(p|D)=\frac{p^{10+a-1}(1-p)^{4+b-1}}{B(10+a,4+b)}
\end{equation}

\textcolor{red}{(revisar todo este ejemplo de la moneda, hay algo que no me quedo claro)}

Now we need to know the values of $a$ and $b$. If we assume that we know nothing about $p$, we can consider our prior as a uniform distribution, this means that $a=b=1$. 
Notice that, from figure \ref{coin0}, our posterior result \eqref{ex4} doesn't agree with the real result (p=0.5). We would expect that our posterior distribution is centered at $p=0.5$ with a very thin distribution. This disagreement can be fixed if we increase our experimental data.\\ $ $\\
\begin{minipage}{\textwidth}
\centering
\includegraphics[height=8cm]{Figures/coin0.png}
\captionof{figure}{\footnotesize{Prior and posterior distributions for the coin example. The black line corresponds with the known real value of $p$.}}
\label{coin0}
\end{minipage}
\\$ $ \\
Solving the integral in \eqref{ex} with \eqref{ex1} and \eqref{ex4} we arrive at the final result obtained in the previous section
\begin{equation}
P(2heads|D)=\frac{B(13,5)}{B(11,5)}=0.485
\end{equation}
%
%
%
\subsection{Updating the probability distribution for a model} 

As seen in the coin example, we couldn't arrive to the real value of $p$ because we didn't have enough data. If we want to be closer to the real value, we would have to keep flipping the coin until the amount of data was big enough. Let's continue with the coin example: after throwing it 100 times we obtain, let's say, 56 heads, or after throwing it 500 times we obtain 246 heads, we should obtain a tinier distribution with the center close to $p=0.5$ (see fig. \ref{coin1}). Given this, it is clear that, in order to confront a parameter model and be more accurate about the most probable (or ``real") value of said parameter, it is necessary to increase the amount of data (and the precision) in any experiment. 
%For example, in the cosmological context, there are a lot of experiments like the ones associated with the barionic acoustic oscilations (BAO), the cosmic microwave background (CMB), the neutrinos background radiation, the redshift survey, etc. (if you are interested in knowing a pedagogical explanation of the different experiments, you can look for \cite{observ}), that can help us to confront our model's parameters (or models) and discriminate them in a more accurately way. 
Then, what we have here are some model's parameters that have to be confronted with different set of data. This can be done in two ways: first one is, regarding the sum of all the sets of data that we have; second one is, consider each data set as the new data, but our prior information has to be set by what we know about the previous information. The important thing in Bayesian statistics is that it doesn't matter which one of the 2 possibilities we choose. In the coin toss example it means that it is equivalent to start with the prior given in figure \ref{coin1}-a and considering the 500 data we can arrive at the posterior \ref{coin1}-d, or start with \ref{coin1}-c as our prior and consider the last 400 data to obtain the same posterior \ref{coin1}-d. \\ $ $ \\
\begin{minipage}{\textwidth}
\centering
\includegraphics[height=10cm]{Figures/coin1.png}
\captionof{figure}{\footnotesize{Posterior distributions of $p$ when our data is increased. Notice that while we continue increasing the experimental results, the posterior distribution starts to be more localized near the real value $p=0.5$.}}
\label{coin1}
\end{minipage}
\\$ $ \\

In fact, if we rewrite Bayes theorem so that all probabilities are explicitly dependent on some prior information $I$
\begin{equation}\label{BayesTI}
P(\theta,H|DI)=\frac{P(\theta,H|I)P(D|\theta,HI)}{P(D|I)}
\end{equation}
and then we consider a new set of data $D'$, letting the old data become part of the prior information $I'=DI$, we arrive at \cite{AlanH}   
\begin{equation}
P(\theta,H|D'I')=\frac{P(\theta,H|I)P(DD'|\theta,HI)}{P(DD'|I)}=P(\theta,H|[DD']I)
\end{equation}
where we can explicitly see the equivalence of the 2 different options. 
\subsection{About the Likelihood}

We mentioned that the evidence in the Bayes theorem is usually not important when we try to do any inference procedure in the parameter space of a single model. Then, we can fix it as $P(D)=1$ without lost of generality. If we ignore the prior\footnote{It is expected that the real value of a given parameter is independent of the prior.} we can identify the likelihood with $P(D|H)=L(D;H)$ and thus, by maximizing it, we can find the most probable model (or model's parameters) for the given data. However, having ignored $P(D)$ and the prior, this approach cannot give a good of fit and thus cannot give an absolute probability for a given model, but it can give relative probabilities. On the other side, it is possible to report results independently of the prior by using the \textit{Likelihood ratio}. The likelihood at a particular point in the parameter space can be compared with the one that best fit the observations, $L_{max}$. Then we can say that a model is acceptable if the likelihood ratio
\begin{equation}
\Lambda=-2\ln\left[\frac{L(D;H)}{L_{max}}\right]
\end{equation}
is bigger than a given value.

On the other side, let us assume we have a posterior distribution, which is single-peaked. We consider that $\hat \theta$ is the peak of the distribution (most probable value) or the mean
\begin{equation}
\hat \theta =\int d\theta \theta P(\theta,H|D).
\end{equation}
If our model is well-specified and the expectation value of $\hat \theta$ corresponds with the real value $\theta_0$
\begin{equation}
\langle\hat \theta\rangle=\theta_0,
\end{equation}
then we say that $\hat \theta$ is \textit{unbiased}. Considering a Taylor expansion of the \textit{log likelihood}
\begin{equation}
\ln L(D;H)=\ln L(D;H_0)+\frac{1}{2}(\theta_\alpha-\theta_{0\alpha})\frac{\partial^2\ln L}{\partial\theta_\alpha \partial\theta_\beta}(\theta_\beta-\theta_{0\beta})+...
\end{equation}
where $H_0$ corresponds with the real model (and/or parameter of the model). In this manner, we have that the likelihood can be expressed as a multi-variable likelihood given by 
\begin{equation}\label{GLik}
L(D;H)=L(D;H_0)\exp \left[-\frac{1}{2}(\theta_\alpha-\theta_{0\alpha})H_{\alpha\beta}(\theta_\beta-\theta_{0\beta})\right]
\end{equation}
where 
\begin{equation}
H_{\alpha\beta}=\frac{\partial^2\ln L}{\partial\theta_\alpha \partial\theta_\beta}
\end{equation}
is called the \textit{Hessian matrix} and it controls whether the estimates of $\theta_\alpha$ and $\theta_\beta$ are correlated or not. If it is diagonal, these estimates are uncorrelated.

The above expression for the likelihood is a good approximation as long as our posterior distribution possesses a single-peak. However, in a general way, the likelihood may not be well described by a gaussian expression at levels which set the interesting credibility levels. It is worth mentioning that, if the data errors are normally distributed, then the likelihood for the data will be a Gaussian function as well.  In fact, this is always true if the model is linearly dependent on the parameters \cite{LiV}. On the other side, if the data is not normally distributed we can resort to the central limit theorem. In this way, the central limit theorem will tell us that the resulting distribution will be best approximated by a multi-variate Gaussian distribution \cite{LiV}.
\subsection{Justifying the neglect of the priors}

In this section we are interested in justifying the neglection of the prior in the Bayes theorem. For this, we follow the example given in \cite{RobT}. In this example there are 2 persons, A and B, that are interested in the measurement of a given physical quantity $\theta$. A and B have different prior beliefs regarding the possible value of $\theta$. This discrepancy could be given by the experience, such as the possibility that A and B have made the same measurement at different times. Let us denote their priors by $P(\theta|I_i)$, $(i=A,B)$, and let us assume that they are described by two Gaussian distributions with mean $\mu_i$ and variance $\Sigma_i^2$. Now, A and B make a measurement together and they obtain the value $\theta_0=m_1$. If we consider that the experiment is such that our parameters are uncorrelated we can rewrite \eqref{GLik} as
\begin{equation}\label{LikG}
L(D;HI)=L_0\exp\left[-\frac{1}{2}\frac{(\theta-m_1)^2}{\sigma^2}\right].
\end{equation}
By replacing the hypothesis $H$ by the continuous variable $\theta$ the Bayes theorem for the model of A and B becomes
\begin{equation}
P(\theta|m_1)=\frac{L(m_1;\theta I_i)P(\theta|I_i)}{P(m_1|I_i)}
\end{equation}
where we have used the notation given in \eqref{BayesTI}. Then, the posterior of A and B are (again) Gaussian with mean
\begin{equation}
\hat \mu_i = \frac{m_1+(\sigma/\Sigma_i)^2\mu_i}{1+(\sigma/\Sigma_i)^2}
\end{equation}
and variance 
\begin{equation}
\tau_i^2=\frac{\sigma^2}{1+(\sigma/\Sigma_i)^2}, \ \ (i=A,B)
\end{equation}
Thus, if the likelihood is more informative than the prior i.e. $(\sigma/\Sigma)\ll 1$ the posterior means of A and B will converge towards the measured value, $m_1$. As more and more data are obtained one can simply replace the value of $m_1$ in the above equation by the mean $\langle m\rangle$ and $\sigma^2$ by $\sigma^2/N$ \textcolor{red}{(que es sigma minuscula?)}. Then, we can see that the initial prior $\mu_i$ of A and B will progressively be overridden by the data. This process is ilutrated in figure \ref{gausian1}.
\begin{minipage}{\textwidth}
\centering
\includegraphics[height=4.5cm]{Figures/g1.png}
\includegraphics[height=4.5cm]{Figures/g2.png}
\captionof{figure}{\footnotesize{Converging views in Bayes inference, taken from \cite{RobT}. A and B have different priors $P(\theta|I_i)$ about a value of $\theta$ (panel (a)). Then, they observe one datum with likelihood $L(\theta;HI)$ (panel (b)), after which their posteriors $P(\theta|m_1)$ (panel (c)) are obtained. Then, after observing 100 data, it can be seen how both posteriors are practically indistinguishable (panel (d))}}
\label{gausian1}
\end{minipage}
\subsection{Chisquare and goodness of fit}

We mentioned that it is necessary to maximize the likelihood in order to obtain the most probable model (or model parameters) given the data. If we consider the Gaussian approximation given in \eqref{LikG} we can see that the likelihood will be maximum if the quantity
\begin{equation}\label{chi2}
\chi^2\equiv(\theta_\alpha-\theta_{0\alpha})H_{\alpha\beta}(\theta_\beta-\theta_{0\beta})
\end{equation}
is minimum. The quantity $\chi^2$ is usually called \textit{chi-square} and is related to the Gaussian likelihood via $L=L_0e^{-\chi^2/2}$. We can say that the maximizing of a Gaussian likelihood procedure and the minimizing of a chisquare procedure are equivalent. However, as we mentioned before, there are some circumstances where the likelihood can't be well specified by a Gaussian distribution, in those cases the chi-square and the likelihood are no longer equivalent. 

We can consider a probability distribution for different values of $\chi^2$ around its minimum. This is the $\chi^2$ distribution for $v=n-M$ degrees of freedom where $n$ is the number of independent data points and $M$ the number of parameters. Hence, we can calculate the probability that an observed $\chi^2$ exceeds by chance a value $\hat \chi$ for the correct model. This probability is given by \cite{NR} $Q(v,\hat\chi)=1-\Gamma(v/2,\hat\chi/2)$, where $\Gamma$ is the incomplete Gamma function. Then, the probability that the observed $\chi^2$ (even the correct model) is less than a given value $\hat\chi^2$ is $1-Q$. This statement is strictly true if the errors are Gaussian and the model is a linear function of the likelihood, i.e., for Gaussian likelihoods.

If we evaluate the quantity $Q$ in the best-fit chi-square (i.e. its minimum) we can have a measure of the goodness of the fit. If $Q$ is small (small probability) we can interpret it as:
\begin{itemize}
\item The model is wrong and can be rejected
\item The errors are underestimated 
\item The measurement errors are not normally distributed.
\end{itemize}
On the other side, if $Q$ is too large there are some reasons that could cause such overestimation:
\begin{itemize}
\item Errors have been overestimated
\item Data are correlated or non-independent
\item The distribution is non-Gaussian.
\end{itemize}
\subsection{Contour plots and confidence regions}

Once the best fit parameter values are obtained we would like to know if there are confidence regions where other values could be considered as good candidates for our model. The most logical election is to consider values inside a compact region around the best fit value. Then, a natural choice is to consider regions with constant $\chi^2$ boundaries. In the case that $\chi^2$ possesses more than 1 minimum, it is said that we have more than one non-connected confidence region. For multi-variate Gaussian distributions (as the likelihood approximation \eqref{LikG}) these are ellipsoidal regions. In this section we exemplify how to calculate the confidence regions following \cite{LiV}. 

We can consider a little perturbation from the best fit of chisquare $\Delta\chi^2=\chi^2-\chi^2_{best}$. Then we can use the properties of $\chi^2$ distribution to define confidence regions for variations on $\chi^2$ to its minimum. In Table \ref{tableerrors} we can see the typical $68.3 \%$, $95.4\%$ and $99.5\%$ confidence levels as a function of number of parameters for the joint confidence level. In the case of Gaussian distribution (as the likelihood) these correspond to the conventional 1, 2 and 3 $\sigma$ \textcolor{red}{(revisar esta parte y la tabla)}.
\begin{table}[h!]
\centering
\begin{tabular}{||l|l|l|l|l||} 
 \hline
$\sigma$ & $p$ & $M=1$ & $M=2$ & $M=3$\\
\hline
$1\sigma$ & $68.3 \%$ & $1.00$ & $2.30$ & $3.53$\\
$2\sigma$ & $95.4 \%$ & $4.00$ & $6.17$ & $8.02$\\
$3\sigma$ & $99.73\%$ & $9.00$ & $11.8$ & $14.20$\\
\hline
\end{tabular}
\caption{\footnotesize{$\Delta \chi^2$ for the conventional $68.3\%$, $95.4\%$ and $99.73\%$ as a function of the number of parameters for the joint confidence level.}}\label{tableerrors}
\end{table}

The general cooking recipe to compute constant $\chi^2$ confidence regions is as follows: After finding the best fit by minimizing $\chi^2$ (or maximizing the likelihood) and if $Q$ for the best parameters is acceptable, then:
\begin{enumerate}
\item Let $M$ be the number of parameters, $n$ the number of data and $p$ be the confidence limit desired.
\item Solve the equation:
\begin{equation}
Q(n-M,min(\chi^2)+\Delta\chi^2)=p
\end{equation}
\item Find the parameter region where $\chi^2\leq min(\chi^2)+\Delta\chi^2$. This defines the confidence region.
\end{enumerate}
\subsection{Marginalization}

It is clear that a model can (in general) depend on more than one parameter. However, most of this parameters $\theta_i$ may be uninteresting. For example, these parameters can correspond to nuisance parameters like calibration factors or it may be that we are interested in constraints on only one parameter at a time rather than on the joint constraints on 2 or more parameters simultaneously. Then we marginalize over the uninteresting parameters by
\begin{equation}
P(\theta_1,...,\theta_j,H|D)=\int d\theta_{j+1}...d\theta_{m}P(\theta,H|D)
\end{equation}
where $m$ is the total number of parameters in our model and $\theta_1$,...,$\theta_j$ denote the parameters that we are interested in.

\subsection{Fisher Matrix}

Once we have a set of data it is important to know how to accurately estimate parameters. Fisher \cite{Fisher} proposed a way to solve this issue 70 years ago. In this section we review the main results of his work following the procedure given in \cite{LiV}.

First of all, we consider again a gaussian likelihood. As we can notice, the Heissian matrix $H_{\alpha\beta}$ has information on the parameters' errors and their covariance. More specifically, when all parameters are fixed except one (e.g. the ith parameter), the error on that one parameter is $1/\sqrt{H_{ii}}$. These errors are called conditional errors, although they are rarely used.

A quantity that arises naturally with gaussian likelihoods to forecast the precision of a model is the so-called \textit{Fisher information matrix}
\begin{equation}
F_{ij}=-\left\langle \frac{\partial^2 \mathcal{L}}{\partial \theta_\alpha \partial \theta_\beta}\right\rangle
\end{equation}
where 
\begin{equation}
\mathcal{L}=\ln L
\end{equation}
It's clear that $F=\langle H\rangle$. The average is made with observational data. 

As we can see from eq. \eqref{rule4}, when we have independent data sets, the complete likelihood is the product of the likelihoods, and the fisher matrix for independent data sets is the sum of the individual fisher matrices. 

A pedagogical and easy case is the one with one-parameter $\theta_i$ considering a gaussian likelihood. In this scenario we have that
\begin{equation}
\Delta \mathcal{L}=\frac{1}{2}F_{ii}(\theta_i- \theta_{0i})^2
\end{equation}
when $2\Delta\mathcal{L}=1$ and by identifying it with the $\Delta \chi^2$ corresponding to $68\%$ confidence level, we notice that $1/\sqrt{F_{ii}}$ yields the $1-\sigma$ displacement for $\theta_i$. In the general case
\begin{equation}\label{rao}
\sigma_{ij}^2 \geq (F^{-1})_{ij}
\end{equation}
Thus, when all parameters are estimated simultaneously from the data, the marginalized error is
\begin{equation}
\sigma_{\theta_i}\geq (F^{-1})^{1/2}_{ii}
\end{equation}
The beauty of the Fisher matrix approach is that there is a simple prescription for setting it up knowing only the model and measurement uncertainties, and that, under the assumption of a gaussian likelihood, the Fisher matrix is the inverse of the covariance matrix. So, all you have to do is set up the Fisher matrix and then invert it to obtain the covariance matrix (that is, the uncertainties on your model parameters). In addition, if it can be computed quickly, it also enables one to explore different experimental set ups and optimize the experiment.

The whole point of the Fisher matrix formalism is to predict how well the experiment will be able to constrain the parameters of the model before doing the experiment and without even simulating the experiment in any detail. We can then forecast the results of different experiments and look at trade-offs such as precision versus cost. In other words, we can engage in experimental design.

The $\leq$ in \eqref{rao} is called the Kramer-Rao inequality. One can see that the Fisher information matrix represents a lower bound for the errors. Only when the likelihood is normally distributed, the $\leq$ is transformed in $=$. However as we saw in [About the likelihood] a gaussian likelihood is only applicable to some circumstances, being generally impossible to be applied, so the key is to have a good understanding of our theoretical model in such a way that we can construct a gaussian likelihood.

\subsection{Importance Sampling}

We call importance sampling (IS) to the different techniques of determining properties of a distribution by drawing samples from another one. The basic idea of this procedure is to consider that the distribution one draws from should be (for a larger number of samples) representative of the distribution of interest. In such case, we should infer different quantities of it. In this section we review the basic concepts necessary to understand what IS is following \cite{importancesampling}.

Suppose we are interested in computing the expectation value $\mu_f=E_p[f(X)]$, where $p(x)$ is a probability density of a random variable $X$ and the sub-index $p$ means average over the distribution $p$. Then, if we consider a new probability density $q(x)$ that satisfies $q(x)>0$ whenever $f(x)p(x)\not = 0$, we can rewrite the mean value $\mu_f$ as
\begin{equation}
\mu_f = \int f(x)p(x)dx=\int f(x)\frac{p(x)}{q(x)}q(x)dx=E_q[f(X)w(x)]
\end{equation}
where $w(x)=p(x)/q(x)$, and now we have an average over $q$. So, if we have a collection of different draws $x^{(1)},...,x^{(m)}$ from $q(x)$, we can estimate $\mu_f$ using this draws as
\begin{equation}
\hat \mu_f = \frac{1}{m}\sum_{j=1}^m w(x^{(j)})f(x^{(j)})
\end{equation}
If $p(x)$ is known only up to a normalizing constant, the above expression can be calculated  as a ratio estimate
\begin{equation}
\hat \mu_f=\frac{\sum_{j=1}^m w(x^{(j)})f(x^{(j)})}{\sum_{j=1}^mw(x^{(j)})}.
\end{equation}
For the strong law of large numbers, in the limit when $m\rightarrow \infty$ we will have that $\hat \mu_f\rightarrow \mu_f$.

In Bayes analysis it can be useful to compute the ratio between evidences for two different models
\begin{equation}\label{importanceratio}
\frac{P'(D)}{P(D)}=E\left[\frac{P'(\theta,D}{P(\theta,D)}\right]_{P(\theta|D)}\simeq \frac{1}{N}\sum_{n=1}^N\frac{P'(D|\theta_n)P'(\theta_n)}{P(D|\theta_n)P(\theta_n)}
\end{equation}
where the samples $\lbrace\theta_n\rbrace$ are drawn from $P(\theta|D)$.

An important result for importance sampling is that, if we have a new set of data which is broadly consistent with the current data (in the sense that the posterior only shrinks), we can make use of importance sampling in order to quickly calculate a new posterior including the new data.
\subsection{Combining datasets: Hyperparameter method}

Suppose we are dealing with multiple datasets,  $\lbrace D_1,...,D_N\rbrace$, coming from a collection of different surveys $\left\lbrace S_1,...,S_N\right\rbrace$. Of course we can't be sure that, a priori, all our data surveys are consistent with one another, or if there is one or more that are likely to be erroneous. If we were sure that all this datasets are consistent, then it should be enough to update the probability as was seen in [Updating the probability distribution for a model] in order to calculate the new posterior distribution for the parameters that we are interested in. However, because we are usually not sure about this, a way to have any information about how useful is a data survey is by introducing the \textbf{hyperparameter method}. This method was first proposed by \cite{hiperp} and \cite{hiperp1} in order to perform a joint estimation of cosmological parameters from combined datasets and it can be used as long as we can consider every survey independent from each other.

In this section we review the main steps necessary to understand the hyperparameter method following ref. \cite{hiperp1}. If the reader is interested in a more explicit explanation of it, they can consult \cite{hiperp} or \cite{hiperp1}.

The main feature of this process is the introduction of a new set of ``hyperparameters" $\alpha$ in our bayesian procedure in order to avoid extra freedom in the parameter estimation process.  These hyperparameters are equivalent to nuisance parameters in such case we need to marginalize over the hyperparameters $\alpha$ in order to recover the posterior distribution, i.e.
\begin{equation}
P(\theta,H|D)=\frac{1}{P(D)}\int P(D|\theta,\alpha,H)P(\theta,\alpha,H)d\alpha
\end{equation}
where we have used the Bayes theorem. Now, it is necessary for the method to assume that the hyperparameters $\alpha$ and the parameters of interest $\theta$ are independent, i.e. $P(\theta,\alpha,H)=P(\alpha)P(\theta,H)$, it is also necessary to assume that each hyperparameter $\alpha_k$ is independent from other hyperparameters, i.e. $P(\alpha)=P(\alpha_1)P(\alpha_2)...P(\alpha_N)$. In this way we can rewrite the above expression as
\begin{equation}
P(\theta,H|D)=\frac{P(\theta,H)}{P(D|H)}\left[\prod_{k=1}^N\int P(D_k|\theta,\alpha_k,H)P(\alpha_k)d\alpha_k\right]
\end{equation}
Here, the quantity inside square brackets is the marginalized likelihood over hyperparameters $L(D;\theta,H)$, and then we can identify the quantity inside the integration as the individual likelihood $L(D_k;\theta,\alpha,H)$, for every $\alpha_k$ and $D_k$; $P(D|H)$ is the evidence and, in a typical parameter inference procedure, works as a normalized function, i.e. $P(D|H)=\int P(\theta,H)L(D;\theta,H)$. Notice that, by considering $P(a_k)=\delta(\alpha_k-1)$, we rely on the standard approach, where no hyperparameters are used.  

We add this hyperparameters in order to, in a way, weight every dataset and take away the importance of the data that doesn't seem to be consistent with other ones. Then, how can we know whether the data support the introduction of hyperparameters? Of course, if the datasets are consistent, the introduction of hyperparameters could make it difficult to make our estimation or give rise to large uncertainties. A way to answer this question is given by the bayesian evidence.

Suppose that we have two models, one that considers the introduction of hyperparameters, called $H_1$, while the second one doesn't, called $H_0$. The bayesian evidence $P(D|H_i)$ is an important quantity if we are interested in making a comparison between two different models. In fact, by defining the ratio between two Bayesian evidences
\begin{equation}
K=\frac{P(D|H_1)}{P(D|H_0)}
\end{equation}
we can estimate if it's necessary to introduce the hyperparameters to our model using the criteria given in the next table:
\begin{table}[h!]
\centering
\begin{tabular}{||l|l||} 
 \hline
 \textbf{K value} & \textbf{Strenght of evidence} \\ [0.5ex] 
 \hline\hline
 $<1$  & Negative (supporting $H_0$)   \\ 
 \hline
 1 to 3 & Weak \\
 \hline 
3 to 10 & Substantial\\
\hline
10 to 30 & Strong\\
\hline
30 to 100 & Very Strong \\
\hline
$>100$ & Decisive\\ [1ex] 
 \hline
\end{tabular}\label{evidence}
\caption{Criteria for the ratio between two Bayesian evidences, taken from \cite{hiperp2}.}
\end{table}

If we consider a gaussian likelihood and a maximum entropy prior, and assuming that, on average, the hyperparameters' weight are unity, we can rewritte the marginalized likelihood function $L(D;\theta,H_1)$ for model $H_1$ as
\begin{equation}
P(D;\theta,H_1)=\prod_{k=1}^N\frac{2\Gamma(\frac{n_k}{2}+1)}{\pi^{n_k/2}|V_k|^{1/2}}(\chi_k^2+2)^{-\left(\frac{n_k}{2}+1\right)}
\end{equation}
obtaining an explicitly functional form for $K$ given by
\begin{equation}
K=\prod_{k=1}^N\frac{2^{n_k/2+1}\Gamma(n_k/2+1)}{\chi^2_k+2}e^{-\chi_k^2/2}.
\end{equation}
Here, $\chi_k$ is the one given by \ref{chi2} for every dataset and $n_k$ is the number of points contained in $D_k$. Notice that, if we have a set of independent samples for $H_0$, we can compute an estimate for $K$ with the help of equation \eqref{importanceratio}. 
%\subsubsection{Computing Fisher matrices}


%\subsection{Summary}

%In order to make a Bayesian parameter inference we showed that the most important artifact necessary to know is the Bayes theorem. This theorem depend basically of 4 components: the prior, the likelihood, the posterior and the evidence. If we are only interested in the parameter inference procedure for a single model, we saw that we can ignore the posterior and the prior, being only the likelihood important for calculating the posterior (the posterior probability distribution for our parameter). Then, we can consider the most probable value for our parameter the one associated with the maximum value of our likelihood.
%
%In the other side we saw that there are some circumstances in which we can rewrite  

\section{Numerical tools}

In a typical scenario it's not possible to compute the posterior distribution analytically. It's important to know the numerical tools at our disposal that can help us in our parameter estimation. Of course we have several candidates that could help us in this task, but in this section we present only the usual one (and the easiest) used in cosmology: the Markov Chain Monte Carlo (MCMC) with the Metropolis Hasting algorithm (MHA). Additionally, in this section we present some useful details that we have to take into account if we want to make efficient our computation \textcolor{red}{(no se si cambiar la palabra ``computation'')}. 

\subsection{MCMC techniques for parameter inference}

The purpose of a MCMC algorithm is to build a sequence of points (called ``chain") in a parameter space in order to evaluate the posterior of eq. \eqref{BayesT} for the usual case where analytical solutions do not exist or are insufficiently accurate. In this section we review the basic results for this procedure in a simplistic way, but for curious readers it is recommendable to check \cite{mcmc1},\cite{mcmc2}, \cite{mcmc3}, or \cite{mcmc4} only for Markov chains.

A sequence $X_1,X_2,...$ of random elements of some set is a \textit{Markov chain} if the conditional distribution of $X_{n+1}$ given $X_1,...,X_n$ depends on $X_n$ only. In other words, a Markov chain is a process where we can do predictions of the future based only in the information given at the present. An important property of a Markov chain is that they can be shown to converge to a stationary state were successive elements of the chain are samples from the target distribution, in our case it converges to the posterior $P(\theta,H|D)$. In this way we can estimate all the usual quantities of interest from it (mean, variance, etc). The number of points required to get good estimates in MCMC is said to scale linearly with the number of parameters, so this method becomes much faster than grids as the dimensionality increases.

The target density is approximated by a set of delta functions
\begin{equation}
p(\theta,H|D)\simeq \frac{1}{N}\sum_{i=1}^N \delta(\theta-\theta_i)
\end{equation}
Then, the posterior mean is calculated as
\begin{equation}
\langle\theta\rangle=\int d\theta \theta P(\theta,H|D)\simeq \frac{1}{N}\sum_{i=1}^N\theta_i
\end{equation}
where $\simeq$ follows because the samples $\theta_i$ are generated from the posterior by construction. Then, we can estimate any integrals (such as the mean, variance, etc) as
\begin{equation}
\langle f(\theta)\rangle \simeq\frac{1}{N}\sum_{i=1}^N f(\theta_i)
\end{equation}

As was mentioned, in a Markov chain it is necessary to generate a new point $\theta_{i+1}$ in our chain from the present point $\theta_i$. However, as it is expected, we need a criteria for accepting (or refusing) this new point depending on if it turns out to be better for our model or not. On the other side, if this new step is worse than the previous one, we may accept it, since it could be the case that, if we only accept steps with better probability, we could be converging into a local maximum in our parameter space and, therefore, not completely mapping it. The simplest algorithm that contains all this information in its methodology is known as the Metropolis-Hastings algorithm and now we will explain it.
\subsubsection{Metropolis-Hastings algorithm}

In the \textit{Metropolis-Hastings algorithm} (MHA) \cite{metr} it is neccesary to start from a random initial point $\theta_0$, with an associated posterior probability $p_0=p(\theta_0,H|D)$. We need to propose a candidate $\theta_c$ by drawing from the \textit{proposal distribution} $q(\theta_0,\theta_c)$. Then, the probability of acceptance of a new point is given by
\begin{equation}
p(acceptance)=min\left[1,\frac{p_cq(\theta_c,\theta_0)}{p_0q(\theta_0,\theta_c)}\right]
\end{equation}
If the proposal distribution is symmetric the algorithm is reduced to the \textit{Metropolis algorithm}
  \begin{equation}
  p(acceptance)=min\left[1,\frac{p_c}{p_0}\right]
  \end{equation}
In this way the complete algorithm can be expressed by the following steps:
\begin{enumerate}
\item Choose a random initial condition $\theta_0$ in parameter space and compute the posterior distribution
\item Generate a new candidate from a proposal distribution in the parameter space and compute the corresponding posterior distribution
\item Accept (or not) the new point with the help of the Metropolis Hasting algorithm
\item If the point is not accepted, repeat the previous point in the chain
\item Repeat steps 2-4 until you have a large enough chain.
\end{enumerate}

\subsubsection{A first example in parameter inference using a MCMC technique with a MHA}

In order to exemplify the numerical tools learned in this section, let us 
go back to the coin toss example seen in subsection [Bayes theorem, prior and posterior distributions]. Since it's of our interest that the reader understands what is the basic procedure given in this section, let us try to estimate what is the value of $p$ (or region of values for $p$) that best matches our data (the 14 times that the coin was thrown). For it we will calculate the posterior distribution \eqref{ex4} using the MHA. 

As before, we consider a Likelihood given by a binomial distribution \eqref{ex2} and a normal distributed prior \eqref{ex3} ($a=b=1$). As our first ``guess" for $p$ we take $p_0=0.1$. We generate a new candidate $p_c$ as $p_c=p_i+G(p_i,\sigma)$, where $G(p_i,\sigma)$ is our proposed gaussian distribution centered at $p_i$ and with variance $\sigma=0.1$; $p_i$ is the current value of $p$, for our first step is $p_i=p_0$. Then, we compute the Metropolis-Hastings algorithm in a Python code as can be seen in appendix [A]. Our final result,  fig. \ref{posteriord}, was a posterior distribution that matches very well the posterior that was calculated analytically. Notice that we have plotted our $95\%$ confidence regions (black line). 

%\begin{figure}[h!]
\begin{minipage}{\textwidth}
\centering

\includegraphics[height=7cm]{Figures/posterior.png}
\captionof{figure}{\footnotesize{Posterior distribution for our example. We plot the prior distribution (blue), true posterior (dashed-red) and the posterior calculated by the MHA (red). We plot $95\%$ confidence region for the estimation of $p$.}}
\label{posteriord}
\end{minipage}\\
%\end{figure}

\noindent To complete the example we show in figure \ref{chain1} the Markov Chain generated by our code. It's easy to see that the chain oscillates around a middle value. This behaviour is expected due to the fact that we don't have enough data to constrain more accurately the value of $p$.

\begin{minipage}{\textwidth}
\centering
\includegraphics[height=6cm]{Figures/chain1.png}
\captionof{figure}{\footnotesize{Markvov chain. We use $p_0=0.1$ as our first ``guess" for $p$. }}
\label{chain1}
\end{minipage}\\ $ $ \\

Note: In appendix [A] we computed our MCMC algorithm using an explicit code of the MCMC process. However, in Python there are some modules that can help us simplify this task. For example, PyMC is a Python module that implements statistical models and fitting algorithms, including the MCMC algorithm. We use this module at the end of this section .Applying the tools learned in a complete work session.

\subsubsection{Convergence test} 
It is clear that we need a test to know when our chain has converged. However, we need to be warned that the point in our chain is not in a "false convergent point" or a locally maximum point. In this sense, we need that our rule takes into account this possible difficulty. The simplest way (the informal way) to know if our chain is converging is running several chains starting with different proposal initial points for the parameter that we are interested in estimating. Then, if we see, by naked eye, that all the chains seem to converge in a single region of the possible value for our parameter, we may consider that our chains are converging to that region. 

 Taking yet again the example of the coins, we can run several chains for the above example and try to estimate if the value (the region) of $p$ that we found is an stationary value (region). In figure \ref{chain2} we plotted 5 different Markov chains with initial ``guess" condition $p=0.2,0.3,0.5,0.7,0.9$. As we expected from the analytical result, all the chains seem to concentrate near the same value.

\begin{minipage}{\textwidth}
\centering
\includegraphics[height=6cm]{Figures/chain22.png}
\captionof{figure}{\footnotesize{Multiple MCMC. We calculate 5 Markov chains to estimate convergence of our chains.}}
\label{chain2}
\end{minipage}


The convergence method above is very informal and we would like to have a better way to ensure that our result is correct. The classical test used for this is the \textit{Gelman-Rubin} (1992) convergence criterion. This is (following \cite{LicV2}, \cite{AlanH}) by starting with $M$ chains with very different initial points and $N$ points per chain, if $\theta_i^j$ is a point in the parameter space of position $i$ and belonging to the chain $j$, we need to compute the mean of each chain 
\begin{equation}
\langle\theta^j\rangle =\frac{1}{N}\sum_{i=1}^N \theta_i^j
\end{equation}
and the mean of all the chains
\begin{equation}
\langle\theta\rangle =\frac{1}{NM}\sum_{i=1}^N\sum_{j=1}^M\theta_i^j.
\end{equation}
Then the chain-to-chain variance $B$ is
\begin{equation}
B=\frac{1}{M-1}\sum_{j=1}^M(\langle\theta^j\rangle-\langle\theta\rangle)^2
\end{equation}
and the average variance of each chain is
\begin{equation}
W=\frac{1}{M(N-1)}\sum_{i=1}^N\sum_{j=1}^M(\theta_i^j-\langle\theta^j\rangle)^2
\end{equation}
If our chains converge, $W$ and $B/N$ must agree. In fact we say that the chain converges when the quantity
\begin{equation}
\hat R=\frac{\frac{N-1}{N}W+B(1+\frac{1}{M})}{W},
\end{equation}
which is the ratio of the two estimates, approach to unity. A typical convergence criteria is when $\hat R<1.03$. 

\subsubsection{Some useful details}

\textit{About the proposal distribution.} The choice of a proposal distribution $q$ is crucial for the efficient exploration of the posterior. In our example we used a Gaussian-like distribution with a variance (step) $\sigma=0.1$. This value was taken because we explored, by hand, different values for $\sigma$ and we took the one that looked to approach more quickly to the real main value of $p$. If the scale of $q$ is too small compared to the scale of the target (in the sense that the typical jump is small), then the chain may take a very long time to explore the target distribution (left side of the plot) which imply that the algorithm will be very inefficient. As we can see in figure \ref{chainprop} (left side), considering a prior for $p=0.8$, the number of points that we plot are not enough for the system to move to its ``real'' posterior distribution. On the other side, if the scale of $q$ is too large, the chain gets stuck and it does not jump very frequently (right side of the figure) which implies that we will have different "picks" in our posterior distribution.    

\begin{minipage}{\textwidth}
\centering
\includegraphics[height=5.5cm]{Figures/chain2.png}
\includegraphics[height=5.5cm]{Figures/chain3.png}
\captionof{figure}{\footnotesize{Two Markov chains considering different variance for our Gaussian proposal distribution. Left side corresponds to $\sigma=0.003$, while right side corresponds to $\sigma = 0.8$.}}
\label{chainprop}
\end{minipage}\\ $ $ \\

In order to fix this issue in a more efficient way, it is recommendable to run an exploratory MCMC, compute the covariance matrix from the samples, and then re-run with this covariance matrix as the covariance of a multivariate Gaussian proposal distribution. Of course, this process can be computed a couple of times before running the ``real" MCMC.\\

\textit{About the burn-in.} It is important to notice that when we start a chain we will have a region of points outside the stationary region where the chain converges (in our chain we could consider those points as the ones inside the ellipse in figure \ref{chain1}). This early part of the chain (called ``burn-in") must be ignored, this means that the dependence on the starting point must be lost. For it, it is important to have a convergence test which can help us to know when the chain has converged.\\

 \textit{Thinning the chains}.- There are several bayesian statisticians that usually thin their MCMC, this means that they do not prefer to save every step given by the MCMC; instead, they prefer to save a new step each time $n$ steps have taken place. An obvious consequence that follows by thinning the chains is that the amount of autocorrelation is reduced. However, as long as the chains are thinned, the precision for the estimated parameters is reduced \cite{thin}. Thinning the chains can be useful in other kind of circumstances like, for example, if we have limitations in memory. Notice that thinning a chain does not yield incorrect results; it yields correct results but less efficient than using the full chains.       
\\

\textit{Autocorrelation probes}.- A complementary way to look for convergence in a MCMC estimation is by looking for the autocorrelation between the samples. The $lag\ k$ autocorrelation is defined as the correlation between every sample and the sample $k$ steps before. It can be quantified as \cite{autocor}
\begin{equation}
\rho_k=\frac{Cov(X_t,X_{t+k})}{\sqrt{Var(X_t)Var(X_{t+k})}}=\frac{E[(X_t-X)(X_{t+k}-X)]}{\sqrt{E[(X_t-X)^2]E[(X_{t+k}-X)^2]}}
\end{equation}
where $X_i$ is the i-th sample and $X$ is the mean of the samples. This autocorrelation should become smaller as long as $k$ increases (this means that samples start to become independent).\\

\textit{Metropolis Coupled Markov Chain Monte Carlo ($MC^3$)}\textcolor{red}{(No estoy seguro si dejarlo o no)}.- It is easy to see that it could be a little problematic if our likelihoods possess local maxima. The $MC^3$ is a modification of the standard MCMC algorithm that consists in running several Markov Chains in parallel to explore the target distribution for different  ``temperatures", and then simplify the way we sample our parameter space and help us to avoid this local maxima. In this little section we exemplify the basic idea of this algorithm following \cite{mcmcmc}. If you are interested in a more extensive explanation of this algorithm, or a modification to make the temperature of the chains dynamical, please consult reference \cite{mcmcmc}.

We consider a tempering version of the posterior distribution $P(\theta,H,T|D)$
\begin{equation}
P(\theta,H,T|D) \propto L(\vec\theta,D)^{1/T}P(\theta,H)
\end{equation}
where $L$ is the likelihood and $P(\theta,H)$ the prior. Notice that, for higher $T$, individual peaks of $L$ become flatter, making the distribution easier to sample with a MCMC algorithm. Now, we have to run N chains with different temperatures assigned in a ladder $T_1<T_2<...<T_N$, usually taken with a geometrically distributed division, with $T_1=1$. The coldest chain $T_1$ is the one that samples the posterior distribution more accurately and behaves as a typical MCMC. Then, we define this chain as the main chain. The other chains are running in such a way that they can cross local maximum likelihoods easier and transport this information to our main chain.  

The chains explore independently the landscape for a certain number of generations. Then, in a pre-determined interval, the chains are allowed to ``swap" its actual position with a probability
\begin{equation}
A_{i,j}=min\left\lbrace\left(\frac{L(\theta_i)}{L(\theta_j)}\right)^{1/T_j-1/T_i},1\right\rbrace .
\end{equation}
In this way, if a swap is accepted, chains $i$ and $j$ must exchange their current position between them in the parameter space, then chain $i$ has to be on position $\theta_j$ and chain $j$ has to move to position $\theta_i$. 

We can see that, since the hottest chain $T_{max}$ can access to all the modes of $P(\theta,H,T_{max}|D)$ easier, then it can propagate its position to colder chains, to be precise, it can propagate its position to the coldest chain $T=1$. At the same time, the position of colder chains can be propagated to hotter chains, allowing them to explore the entire prior volume.   
\\

\textit{\textcolor{blue}{(Monte Carlo Sequential Importance Sampling (MCSIS) `?ser\'ia bueno agregarlo?)}}
\\ $ $

\textit{More samples.-}The generation of the elements in a Markov chain is probabilistic by construction and it depends on the algorithm that we are working with. The MHA is the easiest algorithm used in bayesian inference. However, there are several other algorithms that can help us fulfilling our mission. For instance, some of the most popular and effective ones, apart of the MHA, are the Gibbs sampling (GB) (see e.g. \cite{gibbs1,gibbs2}), the Hamiltoninan Monte Carlo (see e.g. \cite{hamiltonian1,Hamiltonian2}) or the Adaptative Metropolis-Hastings (AMH) (see e.g. \cite{importance}).\\
 
 \subsection{A first complete session work: Adjusting a straight-line}

In this section we apply everything we have learned until now to the simplest example: adjusting a straight-line. This is, we assume that we have a certain theory where our measurements should be in a straight line. Then, in order to apply our techniques, we simulate several datasets along a given line. One of the principal topics that we want to analyse is the hyperparameter method and how it works, so we will apply our analysis for two different examples: first, we consider that we have 2 datasets taken from the same straight-line but with different errors; while in the second case we consider that we have two datasets but now we simulate both of them from different straight-lines and different errors. In our analysis we used the PyMC3 module [red] implemented in Python. Our complete code can be seen in ref. [ref]. This code is so simple to use and can be modified very easily if a new model would be tested. We recommend the archive called ``new model" where the reader can find a blank project where the data and model can be put and, by running all the notebook, obtain all the analysis that we will see in this section. One can find too in the same archive several notes that will help in programming the model with PyMC3. 

\subsubsection{Case 1}

In this example we start by considering that our measurements for a given theory where a straight-line $y=a+bx$ is the one that is expected to be given by the data shown in figure \ref{data_1}. We call this case \textit{Case 1}. This two datasets, D1 and D2, were generated from the line $y=3+2x$, adding a gaussian error to each data in our datasets. For D1 we add an error with a standard deviation $\sigma_1 = 0.3$, while for D2 we add $\sigma_2 = 0.2$. Then, what we would like to estimate are the parameters of the model, i. e. $a$ and $b$. We will analyse this data with and without the hyperparameter method and discuss in detail our results.

\begin{minipage}{\textwidth}
\centering
\includegraphics[height=8cm]{Figures/data_1.png}
\captionof{figure}{\footnotesize{Datasets $D_1$ and $D_2$ measured by our straight-line theory.}}
\label{data_1}
\end{minipage}


\textit{Case without hyperparameters. Model $H_0$.-} Before we make a Bayesian estimation, it is necessary to specify our priors. As we have seen, a good prior is a non informative one. Suppose we only know the boundary limits for $a$ and $b$ (we can see them by eye in our data). Then we consider the flat priors
\begin{equation}
a \propto U[0,5] \ \ \ \text{and} \ \ \ b \propto U[0,3]
\end{equation}
where $U[\alpha,\beta]$ are uniform distributions with lower limit $\alpha$ and upper limit $\beta$.

If now we consider that there are values for $a$ and $b$ for which our data fits better, then from [eq] we can write our likelihood as
\begin{equation}
L(D;line)\propto \exp\left[-\sum_d \frac{(y_d-y)^2}{2\sigma_d^2}\right]
\end{equation}
where $y_d$ is our data taken from the dataset $D=D_1+D_2$ and $\sigma_d$ its errors.

Now we can generate our MCMC with the MHA. In our analysis we ran 6 chains with 10,000 steps for each one. We ran each chain with a temperature $T=2$ and we thinned them every 50 steps. We can see our results in table \ref{tabla1} and figure \ref{chain}. Notice that there are some regions where the frequency of events in our sample is increased. So we can say that such parameter regions look to be more likely to match with the data. Additionally we compute the Gelman-Rubin criterion for each variable in order to verify that our results converged. We see from table \ref{tabla1} that this number is very similar to 1, so our convergence criterion is fulfilled.

\begin{table}[h!]
\centering
\begin{tabular}{||l|l|l|l||} 
 \hline
 & \textbf{Mean} & \textbf{Std. Dev.} & \textbf{Gelman-Rubin} \\ [0.5ex] 
 \hline\hline
$a$ & 2.982407 & 0.047978 & 1.000200 \\
\hline
$b$ & 1.994251 & 0.013490 & 1.000352\\ [1ex] 
 \hline
\end{tabular}
\caption{\footnotesize{Our means obtained in the Bayesian estimation for model $H_0$. We also calculated the Gelman-Rubin criterion for each parameter.}}
\label{tabla1}
\end{table}

\begin{minipage}{\textwidth}
\centering
\includegraphics[height=5cm]{Figures/chain_new.png}
\captionof{figure}{\footnotesize{Results for our sample in the Markov chains for model $H_0$.}}
\label{chain}
\end{minipage}

Now we need to continue with the autocorrelation plots. As we mentioned, we need that these plots are small as k increases in order to consider that our analysis is converging. We see in figure \ref{autocorr} such plots and notice that our convergence criteria is fulfilled.

\begin{minipage}{\textwidth}
\centering
\includegraphics[height=5cm]{Figures/autocorr_1.png}
\captionof{figure}{\footnotesize{Autocorrelation plots for model $H_0$.}}
\label{autocorr}
\end{minipage}
\\$ $

Finally in figure \ref{contour_1} we show the typical $1-4\ \sigma$ confidence regions. We also plot in red the real value for our parameters. The real value for $a$ and $b$ are inside the curve corresponding to 1 standard deviation of our estimations by our inferential method. Then, in Case 1 we can see that the model $H_0$ looks to be a very good estimation procedure.

\begin{minipage}{\textwidth}
\centering
\includegraphics[height=5cm]{Figures/contour_1.png}
\captionof{figure}{\footnotesize{Confidence regions for our parameters for model $H_0$.}}
\label{contour_1}
\end{minipage}\\ $ $

\textit{Case with hyperparameters. Model $H_1$.-} Now it is time to prove our h. Hyperparameter method. In this case our likelihood can be written from [ref] as
\begin{equation}
L(D;\theta,H_1)=\prod_{k=1}^N\frac{2\Gamma(\frac{n_k}{2}+1)}{\pi^{n_k/2}|V_k|^{1/2}}\left[\frac{(y_d-y)^2}{2\sigma_k^2}+2\right]^{-\left(\frac{n_k}{2}+1\right)}
\end{equation}
Now, same as the last procedure, we compute the posterior with our flat priors and using 6 chains with 10,000 steps for each one. Our results and autocorrelation plots can be seen in table \ref{tab} and figure \ref{autocorr_2}. Comparing tables \ref{tabla1} and \ref{tab} we can notice that both procedures look very similar. In fact, the confidence regions for both approximations, fig. \ref{contour_1} and \ref{contour_2}, are similar as well. So, what method is better? We could say that the method with hyperparameters is as good as the one without them, but in order to be sure of it we need to compute the ratio $K$ between both models. We obtained from [eq] 
\begin{equation}
K = 3
\end{equation}
Then, comparing with table [ref] we can say that the evidence for $H_1$ to be better than $H_0$ is weak. In such a case it should better to work with $H_0$ as we explained before.

\begin{table}[h!]
\centering
\begin{tabular}{||l|l|l|l||} 
 \hline
 & \textbf{Mean} & \textbf{Std. Dev.} & \textbf{Gelman-Rubin} \\ [0.5ex] 
 \hline\hline
$a$ & 2.974059 	 & 0.038583 & 1.000229 \\
\hline
$b$ & 1.995189 & 0.010611 	 & 1.000044\\ [1ex] 
 \hline
\end{tabular}
\caption{\footnotesize{Our means obtained in the Bayesian estimation for model $H_1$. We also calculated the Gelman-Rubin criterion for each parameter.}}
\label{tab}
\end{table}

\begin{minipage}{\textwidth}
\centering
\includegraphics[height=5cm]{Figures/autocorr_2.png}
\captionof{figure}{\footnotesize{Autocorrelation plots for model $H_1$.}}
\label{autocorr_2}
\end{minipage}
\\$ $

\begin{minipage}{\textwidth}
\centering
\includegraphics[height=5cm]{Figures/contour_2.png}
\captionof{figure}{\footnotesize{Confidence regions for the parameters in model $H_1$.}}
\label{contour_2}
\end{minipage}\\ $ $

Finally, in order to exemplify our results, let us plot in figure \ref{stright1} our data with the straight-line inferred by the mean parameters of both models. As we expected our estimation fits well the data for both cases.  

\begin{minipage}{\textwidth}
\centering
\includegraphics[height=8cm]{Figures/stright1.png}
\captionof{figure}{\footnotesize{Our stright-lines inferred by our procedures confronted with the data.}}
\label{stright1}
\end{minipage}

\subsubsection{Case 2}

Now we consider that we have a new set of data and the same theory for the straight-line, but suppose that our measurements is kind of distinct. Suppose that we measure the data given in  figure \ref{data_2}, this data corresponds to considering our dataset $D_1$ and changing $D_2$ by 16 new points generated around the line $y=3.5+1.5x$ with a Gaussian noise and standard deviation $\sigma = 0.5$. So, our datasets are not auto-consistent between them. Let us make again a parameter estimation for our parameters and look for the differences in both procedures.

\begin{minipage}{\textwidth}
\centering
\includegraphics[height=8cm]{Figures/data_2.png}
\captionof{figure}{\footnotesize{Datasets $D_1$ and $D_2$ measured by our straight-line theory.}}
\label{data_2}
\end{minipage}\\ $ $

\textit{Case without hyperparameters. Model $H_0$.-} We followed the same procedure as in Case 1. We computed our posterior and verified that our results converged with the help of the Gelman-Rubin criterion and the autocorrelation plots. Our results can be seen in table \ref{tab2}. Then we plotted our $1-4 \sigma$ confidence regions in figure \ref{contour_3}. It's easy to see that our estimation differs so much from the real parameters in our datasets. Of course this is because we are trying to fit a model with non auto-consistent datasets and then we arrive to incorrect results. Now, let us see what happens in the hyperparameters procedure.    

\begin{table}[h!]
\centering
\begin{tabular}{||l|l|l|l||} 
 \hline
 & \textbf{Mean} & \textbf{Std. Dev} & \textbf{Gelman-Rubin} \\ [0.5ex] 
 \hline\hline
$a$ & 3.528359 	 & 0.056511 & 1.000153 \\
\hline
$b$ & 1.795464 & 0.014116 	 	 & 1.000393\\ [1ex] 
 \hline
\end{tabular}
\caption{\footnotesize{Means obtained in the Bayesian estimation for model $H_0$. We also calculated the Gelman-Rubin criterion for each parameter.}}
\label{tab2}
\end{table}

\begin{minipage}{\textwidth}
\centering
\includegraphics[height=8cm]{Figures/contour_3.png}
\captionof{figure}{\footnotesize{Confidence regions for the parameters in model $H_0$.}}
\label{contour_3}
\end{minipage}\\ $ $

\textit{Case with hyperparameters. Model $H_1$.-} We can see in table \ref{tab3} the results of our estimation. In figure \ref{contour_4} we plotted our posterior. What we can see immediately is that both approximations are very different in our posterior. While for model $H_0$ we obtained a single region far away of the real values of our data, for model $H_1$ we obtained two locally maximum regions near the real values for our datasets.  

\begin{table}[h!]
\centering
\begin{tabular}{||l|l|l|l||} 
 \hline
 & \textbf{Mean} & \textbf{Std. Dev.} & \textbf{Gelman-Rubin} \\ [0.5ex] 
 \hline\hline
$a$ & 3.528359 	 & 0.056511 & 1.000153 \\
\hline
$b$ & 1.795464 & 0.014116 	 	 & 1.000393\\ [1ex] 
 \hline
\end{tabular}
\caption{\footnotesize{Means obtained in the Bayesian estimation for model $H_1$. We also calculated the Gelman-Rubin criterion for each parameter.}}
\label{tab3}
\end{table}

\begin{minipage}{\textwidth}
\centering
\includegraphics[height=8cm]{Figures/contour_4.png}
\captionof{figure}{\footnotesize{Confidence regions for the parameters in model $H_1$.}}
\label{contour_4}
\end{minipage}\\ $ $

Finally, we only need to say which method is better. Given the fact that we know \textit{a priori} the real values of our parameters for this example, we could immediately say that the method with hyperparameters is a better approximation than the case without them. However, we confirm this assumption by calculating the ratio $K$ between both models. We obtain
\begin{equation}
K = 37
\end{equation}
which means that we have a very strong evidence that $H_1$ is better that $H_0$.

\section{Bayesian statistic and Cosmology}

%Along the new age of cosmology, observational experiments have been very helpful to confront, eliminate or refined several cosmological models. On this period it has been necessary to improve sensitivity to the experiments in order to achieve stronger constraints on the different models and model's parameters. For example, on CMB satellite experiments, there was a factor of 10 better in sensitive from \textit{COBE} to $WMAP$ and from $WMAP$ to Planck \cite{cmbex}.  On redshift surveys experiments which are interested to map the universe in redshift space, we can compare the 18,000 redshift bright galaxies that were measured by the CfA2 Redshift Survey on 1995 with the \textcolor{red}{[number]} of galaxies that the Sloan Digital Sky Survey (SDSS) measured on \textcolor{red}{(year)} \cite{observ}. In addition there are several new cosmological observations that pretend to update and make our data more precise and that we will need to confront with our models or model's parameters. Such experiments are, for example \textcolor{red}{(Nuevos experimentos. Explicarlos un poco. Aqu\'i entonces comenzar a hablar de que estos nuevos datos se pueden utilizar para constre\~nir par\'ametros).}


In this section we will present the necessary topics of Cosmology in order to understand the application of Bayesian Statistics. We will use natural units.

\textcolor{blue}{En esta parte estar\'ia bien poner un párrafo donde explique brevemente que la forma en
que se utiliza la estadística bayesiana en Cosmología es fijando
parámetros o escogiendo modelos. Luego explicar que para poder hacer un
análisis bayesiano es necesario conocer bien los modelos, las
observaciones y los datos que tenemos a nuestra disposición. Por tal
motivo en esta sección...bla, bla, bla}

\textcolor{blue}{No veo referencias en las im\'agenes.}
\subsection{The metric.}
To study the Universe we consider that it is homogeneous and isotropic at large scales. This is known as \textbf{Cosmological Principle}. Isotropic means that the galaxies distribution doesn't depend on the direction, and homogeneous means that it is independent of the position. In addition,we will use the formalism of General Relativity. It studies the interaction between the geometry and matter contained in the space-time. The curvature of the space-time produces physical effects on the matter it contains, these effects are associated with a gravitational field. On the other hand, the curvature is related to the matter contained by an energy-momentum tensor. The above can be summarized by saying that matter tells space-time how to curve and, in turn, geometry tells matter how to move. We can write the above in the Einstein equation:

\begin{equation}
\label{einstein}
G_{\mu \nu} = 8\pi G T_{\mu \nu}.
\end{equation}

Where $G_{\mu \nu}$ is the Einstein tensor (geometry of the space-time), $T_{\mu \nu}$ is the energy-momentum tensor (matter contained in the Universe) and $G$ is the gravitational constant \cite{baumann}, \cite{wald}.

The distance between two points in a curved space-time can be measured using:

\begin{equation}
ds^2 = g_{\mu \nu}dx^{\mu}dx^{\nu}.
\end{equation}

Where $g_{\mu \nu}$ is the metric tensor which contains all the information about the structure of the space-time. The values that the indices $\mu$ and $\nu$ can take depend on the dimensions of the space-time.

To study the Universe we use the Friedmann-Lema\^itre-Robertson-Walker metric (\textbf{FLRW}). This describes a homogeneous, isotropic and expanding Universe.

\begin{equation}
\label{m2}
ds^2 = dt^2 -a^2(t)\gamma_{ij}dx^idx^j ,
\end{equation}

where

\begin{equation}
\label{m3}
\gamma_{ij} \equiv \delta_{ij} + \kappa \frac{x_i x_j}{1 - \kappa \left(x_k x^k\right)}.
\end{equation}

In equation (\ref{m3}), $\kappa$ describes the curvature. In (\ref{m2}) $a$ is the scale factor which depends on time. By convention we set $a(t_0) \equiv 1$ today \cite{cambridge}.

\subsection{Friedmann and continuity equations.}
With the Cosmological Principle, the energy-momentum tensor describes a perfect fluid \cite{cambridge}

\begin{equation}
\label{tensorfluidoperfecto}
T_{\mu \nu} = (\rho + P)U_{\mu}U_{\nu} - Pg_{\mu \nu} .
\end{equation} 

Where $\rho$ is the energy density, $P$ is the fluid pressure and $U^{\mu}$ is the 4-velocity relative to the observer. Using equations (\ref{einstein}) and (\ref{tensorfluidoperfecto}) we can deduce Friedmann and continuity equations. This last equation is given by

\begin{equation}
\label{continuidad}
\dot{\rho} + 3 \frac{\dot{a}}{a} \left(\rho + P\right) = 0.
\end{equation}

Equation (\ref{continuidad}) implies energy conservation. On the other hand, Friedmann equations describe the expansion of the Universe. These are

\begin{equation}
\label{friedmann1}
H^2 = \left(\frac{\dot{a}}{a}\right)^2 = \frac{8 \pi G}{3} \sum_{i}\rho_{i} - \frac{k}{a^2} ,
\end{equation}

\begin{equation}
\frac{\ddot{a}}{a} = -\frac{4 \pi G}{3} \sum_{i}\left(\rho_i + 3P_i\right) .
\end{equation}

Where $H \equiv \frac{\dot{a}}{a}$ is the Hubble parameter. Using the dimensionless parameters \footnote{$\rho_{crit}$ is the condition to have a flat Universe.}

\begin{eqnarray}
\Omega_{i} &\equiv& \frac{\rho_i}{\rho_{crit}},\\
\rho_{crit} &=& \frac{3H_0^2}{8 \pi G}
\end{eqnarray} 

we can rewrite (\ref{friedmann1}) as

\begin{equation}
\frac{H^2}{H_0^2} = \Omega_r a^{-4} + \Omega_m a^{-3} + \Omega_k a^{-2} + \Omega_{\Lambda} .
\end{equation}

Where $\Omega_r$ is the radiation dimensionless density parameter, $\Omega_m$ corresponds to the matter, $\Omega_k$ with curvature and $\Omega_{\Lambda}$ corresponds to Cosmological Constant. $H_0$ is the Hubble parameter value today.

\subsection{Content of the Universe.}

\begin{itemize}
	\item \textbf{Matter:} It has no pressure and its energy density takes the form $\rho \propto a^{-3}.$ Matter can be baryons (ordinary matter) or dark matter. This dark matter is proposed to explain a lot of observations like the dynamics of the galaxies in Coma cluster or the rotation curves of galaxies. This kind of matter only interacts gravitationally with the rest of the Universe.
	\item \textbf{Radiation:} It is everything that meets the relation $P = \frac{1}{3} \rho$. This implies a density of the form $\rho \propto a^{-4}$. We consider photons and neutrinos as radiation.
	\item \textbf{Dark Energy:} We don't know what it is but we propose it in order to explain the accelerating expansion of the Universe. Dark energy can be vacuum energy or cosmological constant (this is a correction in Einstein equations $-\Lambda g_{\mu \nu}$).
\end{itemize}

We can describe the above using the following state equation

\begin{equation}
\label{ecdeestado}
\omega = \frac{P}{\rho}.
\end{equation}
(\ref{ecdeestado}) is called bariotropic equation because the density only depends on pressure.

\textcolor{blue}{Bariotropic o barotropic?}

\textcolor{blue}{Referenciar la tabla que aparece después de la ecuación 74.}

\begin{table} [htbp]
	\begin{center}
		\begin{tabular}{|l|l|}
			\hline
			Component & $\omega$\\
			\hline
			Matter & 0 \\
			\hline
			Radiation & $\frac{1}{3}$\\
			\hline
			Cosmological constant & -1 \\
			\hline
		\end{tabular}
	\end{center}
\end{table} 

\subsection{Cosmological observables and parameters.}
To explain the Universe we use the dimensionless density parameter of each component ($\Omega_i$), the Hubble constant $H_0$ and the optical depth to
scattering $\tau$ (the probability that a given photon scatters once). Although there are more parameters that can be used, these can be derived from the previous ones.

In the beginning of this section we mentioned that the FLRW metric describes a Universe with some curvature but the cosmological observations indicate a flat Universe. So the curvature density parameter is zero $\Omega_k = 0$.

\textcolor{blue}{Podría decir que más
adelante se usará estadística bayesiana para obtener que las observaciones
son cero (si es que quiere agregar este ejemplo)}

\textbf{Hubble Constant:} It is the slope between the recessional velocity and the proper distance from the galaxy to the observer found by Edwin Hubble in 1929. The Hubble Space Telescope found \cite{hubble2016}:

\begin{equation}
H_0 = 73.24 \pm 1.74 kms^{-1} Mpc^{-1}.
\end{equation}

On the other hand, Planck Collaboration found another value \cite{planck}:
	
\begin{equation}
H_0 = 67.3 \pm 1.2 kms^{-1} Mpc^{-1}.
\end{equation}
	
The discrepancy between these values ​​is still a research issue.

\textbf{Supernovas:} The most accepted explanation is that they are white dwarfs in which thermonuclear processes occur. Empirically, the peak luminosity of the type Ia supernovas (SNIa) can be used as a distance indicator using the relation between redshift and distance\cite{parametros}. 

Using them, the Supernova Cosmology Project and High-z Supernova Search Team both found evidence that the Universe is expanding \cite{supernova1}, \cite{supernova2}, \cite{supernova3}. When this results and CMB data are combined we obtain \cite{parametros}:

\begin{equation}
\Omega_m \approx 0.3,
\end{equation}

\begin{equation}
\Omega_{\Lambda} \approx 0.7 .
\end{equation}

This means that matter (ordinary and dark) occupies $30 \%$ of the content of the Universe and dark energy occupies the remaining $70 \%$. 

\textbf{Cosmic Microwave Background (CMB):} Discovered in 1965, they are the radiation that permeates all the Universe. Before recombination, baryons and photons were tightly coupled. After uncoupling, baryons collapsed due to the effects of gravity and the photons were free and they started traveling. This radiations contains information of last scattering epoch, gravitational lensing, among others. This is because CMB has been traveling throughout the Universe and the interaction with everything they have found in their path leaves a trace that can be detected until our days.

CMB shows anisotropies that are studied using an angular power spectrum in terms of spherical harmonics.

\begin{equation}
T(\theta, \phi) = \sum_{lm} a_{lm} Y_{lm} \left(\theta, \phi \right) .
\end{equation}

Where $a_{lm}$ is constant and $l$ is the multipole moment. This power spectrum is very similar to black body radiation with a temperature $T = 2.725K$ \cite{CMB1}, \cite{CMB2}.

\begin{figure}[h]
	\centering
	\includegraphics[width=12cm]{FiguresCosmo/espectro_temperatura_CMB.png}	
	\caption{CMB anisotropies power spectrum. x axis changes from logarithmic scale to linear scale in $l = 50$. Solid line shows the best fit considering $\Lambda CDM$. Source: \cite{parametros}.}
	\label{CMBgraf} 
\end{figure}

\textbf{Baryon Acoustic Oscillations (BAO's):} We know Universe was almost uniform in the beginning, it contained small fluctuations where the density matter was greater than the average. The gravitational force made these denser regions collapse \textcolor{red}{little by little} until this happened at large scales. But radiation pressure pushed back these regions against the collapse. The pressure causes the baryons and photons to move away from the overdensity region forming a spherical wave. Since dark matter only interacts gravitationally, it remains in the wave center. After photon decoupling there wasn't radiation pressure so only remained overdensity in the wave center and a shell of baryonic matter at a fixed radius. This radius is called sound horizon and is used as a standard unit of distance.

The experiments that showed evidence of BAO's were the 2-degree Field (2dF) Galaxy Redshift Survey \cite{BAO3} and the Sloan Digital Sky Survey (SDSS) \cite{BAO4}. The experiment Baryon Oscillation Spectroscopic Survey (BOSS) found that dark energy state equation is $\omega = 1 \pm 0.06$.

\begin{figure}[h]
	\centering
	\includegraphics[width=10cm]{FiguresCosmo/BAO_cartoon.jpg}	
	\caption{BAO's and sound horizon cartoon. Source: \url{http://www.astro.ucla.edu/~wright/BAO-cartoon.jpg.}}
\end{figure}

\subsection{What do the observations say?}
In the following table we show the values of the parameters (described above) obtained by the experiments \cite{parametros}:

\begin{table} [htbp]
	\begin{center}
		\begin{tabular}{|l|l|l|l|}
			\hline
			Parmeter & Planck+WP+highL & Planck+WP+highL+BAO & WMAP9+eCMB+BAO\\
			\hline
			$\Omega_b h^2$ & $0.02207 \pm 0.00027$  & $0.02214 \pm 0.00024$ & $0.02211 \pm 0.00034$\\
			\hline
			$\Omega_c h^2$& $0.1198 \pm 0.0026$ & $0.1187 \pm 0.0017$ & $0.1162 \pm 0.0020$ \\
			\hline
			$n_s$ & $0.958 \pm 0.007$ & $0.961 \pm 0.005$ & $0.958 \pm 0.008$ \\
			\hline
			$\tau$ & $0.091^{+0.013}_{-0.014}$ & $0.092 \pm 0.013$ & $0.079^{+0.011}_{-0.012}$ \\
			\hline
			h & $0.673 \pm 0.012$ & $0.678 \pm 0.008$ & $0.688 \pm 0.008$ \\
			\hline
			$\Omega_m$ & $0.315^{+0.016}_{-0.017}$ & $0.308 \pm 0.010$ & $0.293 \pm 0.010$ \\
			\hline
			$\Omega_{\Lambda}$ & $0.685^{+0.017}_{-0.016}$ & $0.692 \pm 0.010$ & $0.707 \pm 0.010$ \\
			\hline
		\end{tabular}
		\caption{This table shows the values of the cosmological parameters using different experiments.}
	\end{center}
\end{table} 

The data used were obtained from Planck collaboration, Wilkinson Microwave Anisotropy Probe (WMAP), Atacama Cosmology Telescope (ACT), South Pole Telescope (SPT) and eCMB for CMB. For BAO's we have SDSS, BOSS, 6dF and WiggleZ. We assume $\Lambda CDM$ \footnote{We will explain this model later.}, a flat Universe and cosmological constant as dark energy.

\subsection{Experiments.}
In this section we present only the experiments used in the following examples.

\textbf{Planck:} It is a European Space Agency ESA's mission whose main objective is to measure the temperature, polarization and anisotropies of the CMB over the entire sky. These results would allow to determine the properties of the Universe at large scales, the nature of dark matter and dark energy. As well as test inflation theories, determine if the Universe is homogenous or not and obtain maps of galaxies in the microwave \cite{Planck1}, \cite{Planck2}, \cite{Planck3}.\\

\begin{figure}[h]
	\centering
	\includegraphics[width=10cm]{FiguresCosmo/CMB.jpg}	
	\caption{CMB anisotropies obtained by Planck 2018. Source: \url{http://www.esa.int/spaceinimages/Images/2018/07/Planck_s_view_of_the_cosmic_microwave_background2}.}
\end{figure}

\textbf{BOSS:} The Baryon Oscillation Spectroscopic Survey is part of the third stage of the Sloan Digital Sky Survey (SDSS). BOSS was in charge of mapping the spatial distribution of luminous red galaxies (LRGs) and quasars when detecting the mark left by baryon acoustic oscillations in the early Universe, whose objective is to prove the existence of dark energy. BOSS is a spectroscopic collection of redshifts of 1.5 million LRG's and Lyman-alpha absorption of more than 160000 quasars with high redshifts. BOSS is a telescope with spectroscopic instruments \cite{boss}.\\

\textbf{Joint Light-curve Analysis (JLA):} It is a collaboration to analyze the data from SDSS-II and SNLS experiment \cite{jla}. SDSS-II is the previous version of BOSS and SNLS is the SuperNova Legacy Survey. SNLS used the Canada-France-Hawaii telescope (CFHT), located in Hawaii, to detect type Ia supernovas at high redshift \cite{snls}. The collaboration used 740 samples.

\begin{figure}[h]
	\centering
	\includegraphics[width=10cm]{FiguresCosmo/snls_fig2.png}	
	\caption{Hubble diagram from 740 type Ia supernovae. Published by SNLS and SDSS Source: \cite{snls}.}
\end{figure}

\subsection{$\Lambda$ Cold Dark Matter ($\Lambda$CDM).}
Also know as the Cosmological Standard Model, it is the most accepted model to describe the Universe. This model considers an expanding flat Universe that is fourteen billion years old. 

The components in this model are \cite{liddle}:

\begin{itemize}
	\item \textbf{Radiation:} It is the CMB.
	
	\begin{equation}
	\Omega_{rad 0}h^2 \simeq 2.47 \times 10^{-5} .
	\end{equation}
	
	\item \textbf{Relativistic:} We assume the existence of a Cosmic Neutrino Background.
	
	\begin{equation}
	\Omega_{rel 0}h^2 \simeq 4 \times 10^{-5} .
	\end{equation}
	
	\item \textbf{Baryons:} They are the ordinary matter. They represent approximately the $4 \%$ of the total content of the Universe.
	
	\item \textbf{Dark Matter:} It has no other interactions than gravitational with the rest of the Universe. This model considers cold dark matter, this means that it is non-relativistic at the moment they stop interacting with the rest of the matter.
	
	\begin{equation}
	\Omega_{dm 0} \simeq 0.3
	\end{equation}
	
	\item \textbf{Cosmological constant:} We consider it as dark energy to explain the accelerating expansion of the Universe. The idea of Cosmological Constant comes out as a correction to Einstein equations.
\end{itemize}

\subsection{Cosmological example.}

\textcolor{blue}{Deber\'ias poner m\'as ejemplos de cosmolog\'ia e inferencia de par\'ametros. S\'olo
pones tu parametrizaci\'on CPL, que por cierto nunca la mensionas antes.
Considero que hay ejemplos que deben estar de caj\'on, como es el de LCDM
con y sin curvatura (como lo que yo ten\'ia despu\'es de esta parte).
Tal vez tambi\'en mostrando como funcionan las cosas con diferentes datos,
por ejemplo para H, y mostrar por qu\'e los hiperpar\'ametros podr\'ian ser
interesantes. Finalmente, como un ejemplo extra puedes dejar tu
parametrizaci\'on CPL pero explicando un poco de donde sale y por qu\'e se
suele proponer.
}

In this section we show a few examples of what was learned in previous sections applied on Cosmology. We used the CPL parameterization of dark energy state equation \footnote{For more information see references \cite{CPL1} and \cite{CPL2}} 

\begin{equation}
\omega = \omega_0 + \omega_a\left(\frac{z}{1 + z}\right) = \omega_0 + \omega_a\left(1 - a\right).
\end{equation}

Where $z$ is the redshift, $a$ the scale factor, $\omega_0$ and $\omega_a$ are real numbers.

In the following graphs we fit the parameters in a cosmological model with cold dark matter and the CPL parameterization (dark energy) using the data from Planck collaboration (CMB), BOSS (BAO's) and JLA (Supernovas). \\

\begin{figure}[h]
	\centering
	\includegraphics[width=10cm]{FiguresCosmo/Ok_Om_owaCDM_todos.pdf}	
	\includegraphics[width=10cm]{FiguresCosmo/wa_Om_owaCDM_todos.pdf}
	\includegraphics[width=10cm]{FiguresCosmo/w_Om_owaCDM_todos}
	\caption{Constrictions to cosmological parameters usign observational data. Vertical axis corresponds to matter density parameter. In horizontal axis we have curvature density parameter, $w_a$ and $w_0$ are the CPL parameters.}
	\label{cosmo1}
\end{figure}

In figure \ref{cosmo1}, it can be seen that increasing the number of data reduces the size of the confidence regions. We show, in the tree graphs, the confidence region to 1,2-$\sigma$. For example, if we see the first graph, using only BAO's data we see that the confidence region is bigger than that the obtained using BAO's plus Plank data. In the end, we use the data from Planck, BOSS and JLA to constrain the matter density parameter and curvature density parameter. We found that $\Omega_k \approx 0$ this means that we have a flat Universe (as we expected). We did the same with the CPL parameterization ($\omega_0$ and  $\Omega_a$) in horizontal axis. In vertical axis we have the total matter density parameter.

\subsection{A first example in parameter inference for Cosmology}

The simplest way to understand how all these concepts can be useful in cosmology is applying them to an example. We consider the typical example in cosmology for parameter inference, which is, we estimate the value of the Hubble parameter $H_0$ at our present time and the density of matter in the Universe $\Omega_m$, considering a $\Lambda CDM$ cosmology. In this section we present the results of a complete work session. We wrote our own Python code using the PyMC Python's module [ref]. For interested readers, the code can be seen in appendix [B]. Notice that this can be used not only in cosmology but also in whatever model that you most prefer; what you need to do in order to prove a new model is only specify it in "pm.model()". 

%\subsubsection{What is the model and the theory?}
%
%The standard $\Lambda CDM$ cosmology considers a flat Universe which contains around $\sim 31\%$ of ordinary matter plus dark matter and $\sim 68\%$ of dark energy. In this $\Lambda CDM$ model it is consider that the matter component of the universe follows an equation of state given by $p=0$, while the dark energy is a cosmological constant, i.e. $p=-\rho$. Considering this components, the Universe's dynamics would be given by the Friedmann equation
%\begin{equation}
%H^2\equiv\left(\frac{\dot a}{a}\right)=H_0^2[\Omega_m(1+z)^3+\Omega_{\Lambda}]
%\end{equation}
%and the acceleration equation\footnote{We do not show this equation because it is not necessary for our estimation and we do not want to confuse the reader.}. Here $H$ is the Hubble parameter, $H_0$ corresponds with the Hubble parameter at present, $\Omega_m$ and $\Omega_\Lambda$ is the matter and dark energy densities at our epoch and follows the constraining condition $\Omega_m+\Omega_\Lambda=1$, and $z$ is the redshift which is associated with a time parameter; $z=0$ is at present. Notice that thanks to the constraining condition we can rewrite $\Omega_\Lambda=1-\Omega_m$ and then we can reduce by one the number of parameters in the model. 

\subsubsection{The observables and the data}

It is possible to measure $H(z)$ by using what it is known as the \textit{cosmic chronometer} approach \cite{Hz}. We use the data reported in \cite{Hzdata} as our data for our estimation. A plot of them can be seen in figure \ref{HzData}.

\begin{minipage}{\textwidth}
\centering
\includegraphics[height=8cm]{Figures/Hzdata.png}
\captionof{figure}{\footnotesize{Multiple MCMC. We calculate 5 Markov chains to estimate convergence of our chains.}}
\label{HzData}
\end{minipage}

\subsubsection{Inferring the free parameters of the model}

Now, giving a model and a set of data, we are ready to apply what we have learned until now. First of all, notice that the only free parameters in our model are $\Omega_m$ (or $\Omega_\Lambda$) and $H_0$. We suppose that we don't know anything about our free parameters, in such case a good prior for them is a Uniform distribution. However, in order to simplify our life we consider that we know something about the limit values for both parameters,  say: $\Omega_m\in [0.1,1]$ and $H_0\in [10,100]$. In this way we have as our priors
\begin{subequations}
\begin{equation}
\Omega_m\sim U[0.1,1]
\end{equation}
\begin{equation}
H_0\sim U[10,100]
\end{equation}
\end{subequations} 
\subsection{What is next?}

\textcolor{red}{Una vez de que ya entedimos los procedimientos para la inferencia de par\'ametros, hablar de que el siguiente paso es saber que hay varios c\'odigos que ya hacen todo lo que vimos (tanto la cosmolog\'ia, como la estad\'istica) y que es mejor aprender a moverles que andar haciendo nuestro propio código (quiz\'as)}
%\subsubsection{Cosmological codes}

%Now a days there are a lot of cosmological Boltzmann codes that are free on the web and can help us to prove our cosmological models. The most popular of them are: CMBFAST \cite{cmbfast1}, CMBEASY \cite{cmbeasy}, CAMB \cite{camb1} (some useful references for CAMB: \cite{camb2,camb3,camb4}) and CLASS \cite{class1} (a useful reference for CLASS: \cite{mont1}). All of them are used for calculating the linear CMB anisotropy spectra based on integration over the sources along the photon past light cone.

%\subsubsection{CMBFAST}

%As can be seen in the NASA web-page \cite{cmbfast1}, CMBFAST is a code, written by U. Seljak and M. Zaldarriaga, for calculating the linear cosmic microwave background (CMB) anisotropy spectra based on integration over the sources along the photon past light cone. In this approach the temperature anisotropy is written as a time integral over the product of a geometrical term and a source term. The code can be downloaded as an interface in \cite{cmbfast1} where the user introduce entry parameters required by CMBFAST and when it is run, some of the results are displayed graphically; all of the results are stored in files and made available for the user.

%\subsubsection{CAMB}

%CAMB is a code for anisotropies in the Microwave Background. It was written by Antony Lewis and Anthony Challinor in the f90 language and based on CMBFAST. It can be downloaded in \cite{camb1}. Some useful references for the use of this code can be found in \cite{camb2}, \cite{camb3} and \cite{camb4}.

%\subsubsection{CLASS}

%As can be seen in the wave-page of CLASS \cite{class1}, the purpose of CLASS is to simulate the evolution of linear perturbations in the universe and to compute CMB and large scale structure observables. Its name also comes from the fact that it is written in object-oriented style mimicking the notion of class. The code was written in the C language. It can be downloaded in \cite{class1} and some useful references for the use of the code can be found in \cite{mont1}.

\subsubsection{Statistical codes}

Once our cosmological model is established we need a statistical code which can help us to estimate the free parameters of our model. A first idea could be continuing programing our own MCMC code, but, as it is expected, while the number of free parameters of our model increases, it is more challenging to construct an efficient code. Fortunately there are several MCMC codes free to download on-line that can make this homework taking as our theory the cosmological codes showed above. In this section we review the most common of them.

\textit{Monte Python}.-
Monte Python is a Monte Carlo code for Cosmological Parameter extraction  that can be downloaded in \cite{MP1}. It contains likelihood codes of most recent experiments, and interfaces with the Boltzmann code Class for computing the cosmological observables.

The code contains several sampling methods available: Metropolis-Hastings, Nested Sampling (through MultiNest), EMCEE (through CosmoHammer) and Importance Sampling. If you are interested to work with this parameter inference code you can get help in \cite{mont1} and \cite{MP2}.

\textit{CosmoMC}.- CosmoMC (to download \cite{cosmomc}) is a fortran 2003 MCMC engine for exploring cosmological parameter space. It contains Monte Carlo samples and inportance sampling. It containg likelihoods of most recent experiments, and interfaces with CAMB.

\textit{SimpleMC}.- SimpleMC is a MCMC code for cosmological parameter estimation where only expansion history matters. It was written by Anže Slosar and Jose Vazquez and can be downloaded on \cite{simplemc}

\subsection{Some examples}
\textcolor{red}{Examples of cosmology}
\subsection{Conclusions}
\appendix
\section{A. A simple MCMC python code}

Here we show our MCMC written in Python. This code is very simple and its propose is to help the reader to understand how to programing a MCMC code. However, if you are interested in more sophisticated algorithms you can see the PyMC module of python [ref]. We wrote our code using the jupyter notebook [ref] which is a excelent editor when we have a program no much extense.  

\begin{figure}[h!]
\includegraphics[height=2cm]{Figures/c1.png}
\end{figure}
\begin{figure}[h!]
\includegraphics[height=2.339cm]{Figures/c2.png}
\end{figure}
\begin{figure}[h!]
\includegraphics[height=8.85cm]{Figures/c3.png}
\end{figure}
\begin{figure}[h!]
\includegraphics[height=5.21cm]{Figures/c4.png}
\end{figure}

\begin{thebibliography}{9}
\bibitem{debate} The Shapley - Curtis Debate in 1920, $https://apod.nasa.gov/diamond_jubilee/debate_1920.html$, visited on December 2017
\bibitem{bayeslecture}B.J.K. Kleijin; Bayesian statistic, lecture notes 2015
\bibitem{AlanH} Alan Heavens; Statistical techniques in cosmology; May 2010
\bibitem{RobT} Roberto Trotta; Bayes in the sky: Beyesian inference and model selection in cosmology; March, 2008
\bibitem{LiV} Licia Verde; Statistical methods in cosmology; Nov, 2009
\bibitem{RobTr}Roberto Trotta; Bayes Methods in Cosmology; Jan, 2017
\bibitem{Fisher}Fisher R.A. (1935) \textit{J. Roy. Stat. Soc.} \textbf{98}, 39
\bibitem{NR}Numerical Recipes
\bibitem{mcmc1}anner, M. (1993)
Tools for Statistical Inference, Method for
Exploration of Posterior Distributions and Likelihood Func-
tions.
\bibitem{mcmc2}Gilks, W., Richardson, S. and Spiegelhalter, D. (1996)
Markov Chain
Monte Carlo in Practice.
\bibitem{mcmc3}Gelman, A., Carlin, J., Stern, H and Rubin, D. (1995)
Bayesian Data
Analysis.
\bibitem{mcmc4}oss, Sheldon, (1989)
Introduction to Probability models 4th
Edit.
\bibitem{metr} Metropolis, N., Rosenbluth, A.W., Rosenbluth, M.N., Teller, A.H., Teller, E.: Equation of state
calculations by fast computing machines. J. Chem. Phys.
21
, 1087–1092 (1953)
\bibitem{LicV2} Verde L., astroph/0712.3028 (2007)-
\textit{http://www.behind-the-enemy-lines.com/2008/01/are-you-bayesian-or-frequentist-or.html}
\bibitem{observ}218.163.109.230 et al. (2004–2014); \textit{Observational cosmology -
30h course}.
\bibitem{gibbs1} A. Smith and G. Roberts, J. R. Statist. Soc. B
55
3–23 (1993).
\bibitem{gibbs2} Ilker Yildirim, Bayesian Inference: Gibbs Sampling, August 2012
\bibitem{hamiltonian1}K. M. Hanson, Markov Chain Monte Carlo posterior sampling with the Hamiltonian method, in M. Sonka and K. M. Hanson eds, Medical Imaging: Image Processing
Vol. 4322, Proc. SPIE, pp. 456–467.
\bibitem{Hamiltonian2} Radford M. Neal, MCMC using Hamiltonian dynamics, arXiv:1206.1901v1 [stat.CO]
\bibitem{importance} Surya T. Tokdar and Robert E. Kass, Importance Sampling: A review, DOI: 10.1002/wics.56
%
%
%
%
\bibitem{cmbex} K. N. Abazajian, K. Arnold, J. Austermann, B. A. Ben-
son, C. Bischoff, J. Bock, J. R. Bond, J. Borrill, E. Cal-
abrese,  J.  E.  Carlstrom,  et  al.,  ArXiv  e-prints  (2013),
1309.5383.
\bibitem{cmbfast1}$https://lambda.gsfc.nasa.gov/toolbox/tb_cmbfast_ov.cfm$
\bibitem{cmbeasy}$http://adsabs.harvard.edu/abs/2010ascl.soft07004D$
\bibitem{camb1}$http://camb.info/$
\bibitem{camb2}$http://camb.info/readme.html$
\bibitem{camb3}$http://cosmocoffee.info/viewforum.php?f=11$
\bibitem{camb4} $http://cosmologist.info/notes/CAMB.pdf$
\bibitem{class1} http://class-code.net/
\bibitem{mont1}Zumalacarregui Miguel; \textit{CLASS, hi class and Monte Python basics IFT School on Cosmology Tools}; March 11, 17.
\bibitem{MP1}\textit{http://baudren.github.io/montepython.html}
\bibitem{MP2}Benjamin Audren, \textit{Monte Python documentation, Release 2.2.0}; October 21, 2015
\bibitem{cosmomc}$http://cosmologist.info/cosmomc/$	
\bibitem{simplemc}$https://github.com/ja-vazquez/SimpleMC$
%
%
%
%
%
%
\bibitem{Hz}R. Jimenez and A. Loeb, “Constraining Cosmological Parameters Based on Relative Galaxy Ages,”
ApJ, vol. 573, pp. 37–42, July 2002.
\bibitem{Hzdata}Michele Moresco, Raul Jimenez, Licia Verdec, Andrea Cimatti, Lucia Pozzetti, Claudia Maraston, \textit{Constraining the time evolution of dark energy, curvature and neutrino properties with cosmic chronometers}, Journal of Cosmology and Astroparticle Physics, (2016), arXiv:1604.00183v1 [astro-ph.CO]
\bibitem{autocor} Model checking diagnostic, PyMC 2.3.6 documentation, link: \textit{https://pymc-devs.github.io/pymc/modelchecking.html}
\bibitem{thin}William A. Link and Mitchell J. Eaton, On thinning of chains in MCMC, Methods in ecology and evolution,
\bibitem{mcmcmc}W. D. Vousden, W. M. Farr and I. Mandel,\textit{Dynamic temperature selection for parallel-tempering in Markov chain Monte Karlo simulations}, MNRAS (January 11, 2016) Vol. 455 1919-1937, arXiv:1501.05823 [astro-ph.IM]
\bibitem{hiperp}Lahav O., Bridle S. L., Hobson M. P., Lasenby A. N., $\&$ Sodre L., 2000, MN-RAS, 315, 45
\bibitem{hiperp1}Hobson M. P., Bridle S. L., $\&$ Lahav O., 2002, MNRAS, 335, 377 (HBL
\bibitem{hiperp2}e
ff
reys H.,
The Theory of Probability
, Oxford University Press, 1961.
\bibitem{importancesampling}Surya T Tokdar and Robert E Kass, Importance sampling: A Review, DOI: 10.1002/wics.56

\bibitem{baumann} Baumann D., \textit{Cosmology. Part III Mathematical Tripos.,} University of Cambridge.

\bibitem{wald} Wald R. M., \textit{General Relativity,} The University of Chicago Press (1984).

\bibitem{cambridge} D. Baumann, {\em Cosmology. Part III Mathematical Tripos.}, University of Cambridge.

\bibitem{hubble2016} Riess A. G. et al., 2016, arXiv: 1604.01424.

\bibitem{planck} Ade P.A.R. et al., 2013, arXiv:1303.5076

\bibitem{parametros} Lahav O., Liddle A. R., 2014, arXiv: 1401.1389v1.

\bibitem{supernova1} Riess A. G. et al., 1998, 	arXiv:astro-ph/9805201.

\bibitem{supernova2} Garnavich P. et al., 1998, arXiv:astro-ph/9806396. 

\bibitem{supernova3} Perlmutter S. et al., 1999, arXiv:astro-ph/9805201

\bibitem{CMB1} Scott D.,  Smoot G. F., {\em Cosmic Microwave Background}, University of British Columbia, 2003. \url{http://pdg.lbl.gov/2004/reviews/microwaverpp.pdf} 

\bibitem{CMB2} Eidelman S. et al. (Particle Data Group), 2004, Phys. Lett. B 592, 1.

\bibitem{BAO3} Cole S. et al., 2005, MNRAS 362, 505.

\bibitem{BAO4} Eisenstein D. et al., 2011, Astrophys. J., 633, 560.

\bibitem{Planck1} {\em Planck}, European Space Agency. \url{https://www.esa.int/Our_Activities/Space_Science/Planck}

\bibitem{Planck2} {\em Plack Science Team Home}, European Space Agency. \url{https://www.cosmos.esa.int/web/planck/home}

\bibitem{Planck3}{\em Looking back to the dawn of time Planck.}, European Space Agency. \url{http://people.na.infn.it/~barbarin/MaterialeDidattico/0+approfondimenti%20corso%20Fisica%20astroparticellare/1-CMB/Planck-div.pdf}
	
\bibitem{boss} Dawson K.S. et al., 2013, Astronomical Journal.

\bibitem{jla} Betoule M. et al., 2014, arXiv: 1401.4064v2.

\bibitem{snls} \url{http://irfu.cea.fr/en/Phocea/Vie_des_labos/Ast/ast_technique.php?id_ast=2289}

\bibitem{liddle} A. Liddle, {\em An Introduction to Modern Cosmology}, John Wiley and Sons Ltd, Second Edition (2003).

\bibitem{CPL1} Linden S., Virey J., 2008, arXiv: 0804.0389.

\bibitem{CPL2} Scherrer R. J., 2015, arXiv: 1505.05781.

\end{thebibliography}
\end{document}

